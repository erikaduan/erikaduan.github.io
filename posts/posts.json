[
  {
    "path": "posts/2020-12-31-cleaning-free-text-and-wrangling-strings/",
    "title": "Cleaning free text and wrangling strings",
    "description": "These are some common data cleaning things.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "data cleaning",
      "nlp"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCreating a test dataset\r\nIntroduction to regular expressionsMatch characters\r\nCharacter anchors\r\nCharacter classes and groupings\r\nGreedy versus lazy matches\r\nLook arounds\r\n\r\nImproving comment field readability\r\nExtracting topics of interest\r\nExtracting a machine learning friendly dataset\r\nDifferences between base R and stringr functions\r\nOther resources\r\n\r\nIntroduction\r\nComment fields sit right in between tidy tabular data entries and large text files (i.e. documents) in terms of wrangling effort. They require human naunce to decode and more problematically, both the quality and completeness of comment entries vary depending on individual engagement with reporting requirements.\r\nThis can make it hard to gauge whether wrangling comment fields is a fruitful endeavour (especially when you have multiple other data sources that need examining). Luckily, some knowledge of string manipulations and regular expressions can help simplify this process.\r\n\r\n\r\n#-----load required packages-----  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,  \r\n               tidyverse,  \r\n               DT, # for visualising tables   \r\n               microbenchmark) \r\n\r\n\r\n\r\nCreating a test dataset\r\nLet’s imagine that my local chocolate company, Haighs Chocolates, wants to understand what food critics versus Haighs fans think about their newest product. They send out a bag of free samples with a link to an online survey that asks individuals to rate their chocolates (on a scale of 1 to 10) and provide additional comments.\r\nNote: The code used to create this dataset can be accessed from the Rmd file accompanying this tutorial.\r\n\r\n\r\n\r\n\r\n\r\n#-----quickly visualise the test dataset-----  \r\nsurvey %>%\r\n  head(10) # fields containing html flags are not properly rendered by kable \r\n\r\n\r\n\r\nIntroduction to regular expressions\r\nRegular expressions (or regex) can be thought of as a separate syntax for handling patterns in strings. In R, regular expressions can be directly enclosed inside quotes (\"\" or '') like regular character strings, or explicitly referenced inside regex(). For convenience, I prefer the former approach but the latter approach can help increase code readability.\r\n\r\n\r\n#-----call regular expressions in R-----\r\nmany_apples <- c(\"Apple\", \"apple\", \"APPLE\", \"apples\")\r\n\r\nstr_extract(many_apples, # the string\r\n            \"apples?\") # the pattern i.e. regex  \r\n#> [1] NA       \"apple\"  NA       \"apples\"\r\n\r\n#-----call regular expressions in R using regex()-----\r\n# regex() provides additional arguments\r\n\r\nstr_extract(many_apples, \r\n            regex(\"apples?\", ignore_case = T))  \r\n#> [1] \"Apple\"  \"apple\"  \"APPLE\"  \"apples\"\r\n\r\n# regex() also allows comments to improve regex readability  \r\n\r\nstr_extract(many_apples, \r\n            regex(\"\r\n                  apple  # contains the word apple\r\n                  s?  # contains zero or one of the letter s\r\n                  \" , comments = T))\r\n#> [1] NA       \"apple\"  NA       \"apples\"  \r\n\r\n\r\n\r\nMatch characters\r\nSome sequences of characters have specific meanings. For example, s refers to the letter \"s\" but \\s refers to any type of white space. To call whitespace in R, a second backslash \\ is required to escape special character behaviour i.e. \\\\s.\r\n\r\n\r\n#-----examples of special character sequences-----  \r\nwords_and_spaces <- c(\"a cat\",\r\n                      \"acat\",\r\n                      \"a   cat\",\r\n                      \"a\\ncat\",\r\n                      \"a\\\\ncat\")\r\n\r\n# \"a\\\\s+cat\" calls variations of a...cat separated by one or more whitespaces \r\n# note that the string \"a\\ncat\" also counts because \\n refers to a new line\r\n\r\nstr_extract(words_and_spaces, \"a\\\\s+cat\")  \r\n#> [1] \"a cat\"   NA        \"a   cat\" \"a\\ncat\"  NA      \r\n\r\n# \"\\\\S+\" refers to everything that is not white space (starting from left to right)  \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"\r\n\r\n\r\n\r\nNote: The special characters \\s versus \\S, \\d versus \\D and \\w versus \\W are handy as they allow the extraction of opposite pattern types. For example, \\w refers to any word character whilst \\W and [^\\w] both refer to anything that is not a word character.\r\nCharacter anchors\r\nI feel that the goal of writing good regex is to be as specific as possible. This is why character anchors can be useful (i.e. using ^ and $ to denote the start and end of your string respectively).\r\nIf we revisit the example above, we can see that the presence or absence of character anchors produces very different outputs.\r\n\r\n\r\n#-----impact of character anchors-----    \r\nwords_and_spaces <- c(\"a cat\",\r\n                      \"acat\",\r\n                      \"a   cat\",\r\n                      \"a\\ncat\",\r\n                      \"a\\\\ncat\")\r\n\r\n# \"\\\\S+\" refers to everything that is not white space (from left to right unless specified)  \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"  \r\n\r\nstr_extract(words_and_spaces, \"^\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"  # same output    \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+$\") \r\n#> [1] \"cat\"     \"acat\"    \"cat\"     \"cat\"     \"a\\\\ncat\" # different output      \r\n\r\n\r\n\r\nCharacter classes and groupings\r\nCharacter classes and groupings are handy for extracting specific letter and/or digit combinations. Some special characters found inside character classes and groupings are:\r\nThe operation or is represented by | i.e [a|c]\r\nThe operation range is represented by - i.e. [a-z]\r\nThe operation excludes is represented by ^ i.e. [^a-c]\r\n\r\n\r\n#-----extract patterns using character classes i.e. []-----  \r\nstrange_fruits <- c(\"apple1\",\r\n                    \"bapple2\",\r\n                    \"capple3\",\r\n                    \"dapple4\",\r\n                    \"epple5\",\r\n                    \"aggle0\")\r\n\r\nstr_extract(strange_fruits, \"[a-d]\")\r\n#> [1] \"a\" \"b\" \"c\" \"d\" NA  \"a\"  \r\n\r\n# \"[a-d][^p]\" refers to one character between a and d followed by one character that is not p  \r\n\r\nstr_extract(strange_fruits, \"[a-d][^p]\")\r\n#> [1] NA   \"ba\" \"ca\" \"da\" NA   \"ag\"   \r\n\r\n# \"[0|5-9]\" refers to a number that is zero or a number from 4 to 9\r\n\r\nstr_extract(strange_fruits, \"[0|4-9]\")\r\n#> [1] NA  NA  NA  \"4\" \"5\" \"0\"   \r\n\r\n\r\n\r\n\r\n\r\n#-----extract character using groupings i.e. ()-----     \r\nstrange_fruits <- c(\"apple1\",\r\n                    \"bapple2\",\r\n                    \"capple3\",\r\n                    \"dapple4\",\r\n                    \"epple5\",\r\n                    \"aggle1\")  \r\n\r\nstr_extract(strange_fruits, \"a(pp|gg)le\")\r\n#> [1] \"apple\" \"apple\" \"apple\" \"apple\" NA      \"aggle\"    \r\n\r\n# groups can be referenced by their order of appearance i.e. \\\\1 = first group = (e)  \r\n\r\nstr_extract(strange_fruits, \"(a)(p|g)\\\\2\")\r\n#> [1] \"app\" \"app\" \"app\" \"app\" NA    \"agg\"   \r\n\r\n# (a) is group 1 and can be called using \\\\1    \r\n# (p|g) is group 2 and can be called using \\\\2     \r\n\r\n\r\n\r\nNote: The difference between a character class and a grouping is that a character class refers to only one character (whilst allowing for multiple character forms), whereas a group refers to an entire sequence of characters.\r\nGreedy versus lazy matches\r\nUsing a non-greedy as opposed to greedy match allows you to extract just the first sequence of characters separated by a white space or punctuation mark. This use case is most applicable to trimming strings or extracting file or object names.\r\n\r\n\r\n#-----use cases for greedy matches-----   \r\nmessy_dates <- c(\"Thursday 24th May\",\r\n                 \"Thursday  24th May  \",\r\n                 \"May\",\r\n                 \"May    \")\r\n\r\n# extract the first word in the string      \r\n\r\nstr_extract(messy_dates, \"^\\\\w+\") # * represents zero or more of i.e. a greedy match   \r\n#> [1] \"Thursday\" \"Thursday\" \"May\"      \"May\"   \r\n\r\nstr_extract(messy_dates, \"^\\\\w{0,}\") # * and {0,} are the same  \r\n#> [1] \"Thursday\" \"Thursday\" \"May\"      \"May\"    \r\n\r\nstr_extract(messy_dates, \"^(\\\\S+)\") # also produces the same output  \r\n#> [1] \"Thursday\" \"Thursday\" \"May\"      \"May\"    \r\n\r\n#-----use cases for non-greedy matches-----  \r\nstr_replace_all(messy_dates, \"\\\\s\" , \"-\") # replaces each individual whitespace\r\n#> [1] \"Thursday-24th-May\"    \"Thursday--24th-May--\" \"May\"                  \"May----\"       \r\n\r\nstr_replace_all(messy_dates, \"\\\\s{1,4}\" , \"-\") \r\n#> [1] \"Thursday-24th-May\"  \"Thursday-24th-May-\" \"May\"                \"May-\"       \r\n\r\n# use look arounds (next topic) to replace the whitespace(s) after the first word     \r\n\r\nstr_replace_all(messy_dates, \"(?<=^\\\\S{1,100})\\\\s{1,4}\" , \"-\") \r\n#> [1] \"Thursday-24th May\"   \"Thursday-24th May  \" \"May\"                 \"May-\"     \r\n\r\n\r\n\r\nNote: For further details explaining the regex syntax for the last example, read this stack overflow post.\r\nLook arounds\r\nLook around operations are useful when you are unsure of the pattern itself, but you know exactly what its preceding or following pattern is. I’ve found that the clearest explanation of look around operations comes from the RStudio cheetsheet on string_r and is depicted below.\r\n\r\n\r\nknitr::include_graphics(\"https://github.com/erikaduan/erikaduan.github.io/blob/master/images/2020-12-31_look-arounds.jpg\")  \r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for different types of look arounds-----  \r\nrecipes <- c(\"crossiant recipes\",\r\n             \"apple pie recipe\",\r\n             \"chocolate cake  recipe\", # extra space\r\n             \"cookie receipe\",  # deliberate typo\r\n             \"secret KFC-recipe\", \r\n             \"very secret  McDonalds soft-serve recipe\") # extra space  \r\n\r\n# use positive look-ahead (?=...) to extract the preceding word\r\n\r\nstr_extract(recipes, \"\\\\S+(?=\\\\s*recipes?)\")   \r\n#> [1] \"crossiant\"  \"pie\"        \"cake\"       NA           \"KFC-\"       \"soft-serve\"   \r\n\r\n# use positive look-behind (?<=) to identify the secret recipe   \r\n\r\nstr_extract(recipes, \"(?<=secret\\\\s{1,10})\\\\S+.+\")   \r\n#> [1] NA                            NA                            NA                           \r\n#> [4] NA                            \"KFC-recipe\"                  \"McDonalds soft-serve recipe\"\r\n\r\n\r\n\r\nNote: Positive look-behinds require defined boundary specifications i.e. the operation + needs to be converted into {1,1000}.\r\nImproving comment field readability\r\nWith regex revised, let us return to the Haighs chocolate survey. The first thing we can see is that html tags have been retained inside the comment field and that this field is very long (i.e. difficult to read).\r\nWe can improve the readability of the survey by:\r\nRemoving all html tags using regex.\r\nSeparating phrases into individual fields i.e. columns using separate().\r\n\r\n\r\n#-----examine survey data-----\r\nsurvey %>%\r\n  head(5)   \r\n\r\n#-----remove html tags-----\r\nremove_html_tags <- regex(\"\r\n                          <  # starts with <\r\n                          [^>]+  # contains one or more of all characters excepting > \r\n                          >  # ends with >\r\n                          \", comments = T)\r\n\r\nremove_more_html <- regex(\"\r\n                          \\\\& # starts with &\r\n                          \\\\w+ # contains one or more word characters\r\n                          \\\\; # ends with ;\r\n                          \", comments = T) \r\n\r\nsurvey <- survey %>%\r\n  mutate(comment_field = str_replace_all(comment_field, remove_html_tags, \"\"),\r\n         comment_field = str_replace_all(comment_field, remove_more_html, \"\"))\r\n\r\n#-----examine comment field-----  \r\nsurvey %>%\r\n  select(comment_field) %>%\r\n  head(5) %>%\r\n  knitr::kable()\r\n\r\n\r\n\r\nWe can split the comment field into smaller phrases, separating by punctuation marks or conjunctions.\r\n\r\n\r\n#-----separate comment field into an unknown number of columns of phrases-----    \r\nnmax <- max(str_count(survey$comment_field, \"[:punct:]|and|with|against\")) + 1\r\n\r\nsurvey <- survey %>%   \r\n  separate(comment_field,\r\n           into = paste0(\"Field\", seq_len(nmax)),\r\n           sep = \"[[:punct:]]|and|with|against\", # separate on punctuation or conjunctions  \r\n           remove = F,\r\n  extra = \"warn\",\r\n  fill = \"right\") \r\n\r\n#-----examine comment field-----  \r\nsurvey %>%\r\n  select(-c(respondee, rating, comment_field)) %>%\r\n  head(5) %>%\r\n  knitr::kable()\r\n\r\n\r\n\r\n1\r\nExtracting topics of interest\r\nAfter separating the comment field into individual phrases, I can see that there are references to:\r\nthe cocoa bean grade\r\npresence of caramel or vanilla flavour\r\nchocolate smoothness\r\nhow well the chocolate melts\r\nsugar content/ sweetness level\r\nmalt filling\r\nchocolate coating\r\nInformation about cocoa bean grade is highly structured. This means that extracting the letter following the word “Grade” is sufficient. A similar logic can be applied to extract whether caramel or vanilla flavour or chocolate smoothness was mentioned.\r\n\r\n\r\n#-----extract information about cocoa bean grade, flavour and smoothness-----\r\ntidy_survey <- survey %>%\r\n  select(respondee,\r\n         comment_field) %>% \r\n  mutate(cocoa_grade = str_extract(comment_field, \"(?<=[G|g]rade\\\\s{0,2})[A-C]\"),\r\n         is_caramel = case_when(str_detect(comment_field, \"[C|c]aramel\") ~ \"yes\",\r\n                                TRUE ~ \"NA\"), \r\n         is_vanilla = case_when(str_detect(comment_field, \"[V|v]anilla\") ~ \"yes\",\r\n                                TRUE ~ \"NA\"),\r\n         is_smooth = case_when(str_detect(comment_field, \"[S|s]mooth\") ~ \"yes\",\r\n                               TRUE ~ \"NA\")) \r\n\r\n# note that when using case_when, TRUE cannot be converted into a logical vector  \r\n\r\ntidy_survey <- tidy_survey %>%\r\n  mutate_at(vars(cocoa_grade), ~ replace_na(., \"NA\"))\r\n\r\n# replace NA in cocoa_grade with the character \"NA\" for consistency  \r\n\r\n\r\n\r\nFor more descriptive fields i.e. whether or how the chocolate melts, I find it easier to first extract a matrix of fields containing the topic of interest.\r\n\r\n\r\n#-----extract information about chocolate texture-----\r\nmelt_matrix <- survey %>%\r\n  select_at(vars(respondee,\r\n                 starts_with(\"Field\"))) %>% \r\n  mutate_at(vars(starts_with(\"Field\")),\r\n            ~ replace(., !(str_detect(., \".*\\\\b[M|m]elt.*\\\\b.*\")), NA)) \r\n\r\n# convert fields which do not contain \"melt\" into NA and unite all fields     \r\n\r\nmelt_cols <- str_which(colnames(melt_matrix), \"^Field.+\")\r\n\r\nmelt_status <- melt_matrix %>%\r\n  unite(\"is_melty\", # new column \r\n        all_of(melt_cols), # unite these columns  \r\n        sep = \"\",\r\n        remove = T,\r\n        na.rm = T) # make sure to remove NAs  \r\n\r\n#-----convert responses into factors and recode factor levels-----  \r\nmelt_status$is_melty <- factor(melt_status$is_melty)\r\nlevels(melt_status$is_melty) \r\n\r\nmelt_status <- melt_status %>%\r\n  mutate(is_melty = fct_collapse(is_melty,\r\n                                 \"yes\" = c(\" Easily melts\",\r\n                                           \" Melts well\",\r\n                                           \" Melts easily\",\r\n                                           \" melts in your mouth\"),\r\n                                 \"NA\" = \"\"))\r\n\r\n#-----left join tidy_survey to melt_status-----  \r\ntidy_survey <- tidy_survey %>%\r\n  left_join(melt_status,\r\n            by = \"respondee\")\r\n\r\n\r\n\r\n\r\n\r\n#-----extract information about chocolate sweetness-----  \r\nsweetness_matrix <- survey %>%\r\n  select_at(vars(respondee,\r\n                 starts_with(\"Field\"))) %>% \r\n  mutate_at(vars(starts_with(\"Field\")),\r\n            ~ replace(., !(str_detect(., \".*\\\\b[S|s](weet)|(ugar).*\\\\b.*\")), NA)) \r\n\r\n# convert fields which do not contain \"sweet\" or \"sugar\" into NA and unite all fields     \r\n\r\nsweetness_cols <- str_which(colnames(sweetness_matrix), \"^Field.+\")\r\n\r\nsweetness_status <- sweetness_matrix %>%\r\n  unite(\"is_sweet\", # new column \r\n        all_of(sweetness_cols), # unite these columns  \r\n        sep = \"\",\r\n        remove = T,\r\n        na.rm = T) # make sure to remove NAs  \r\n\r\n#-----convert responses into factors and recode factor levels-----  \r\nsweetness_status$is_sweet <- factor(sweetness_status$is_sweet)\r\nlevels(sweetness_status$is_sweet) \r\n\r\nsweetness_status <- sweetness_status %>%\r\n  mutate(is_sweet = fct_collapse(is_sweet,\r\n                                 \"yes\" = c(\"filled core may be too sweet for some\"),\r\n                                 \"no\" = c(\" low sugar content \",\r\n                                          \" not so sweet  I enjoyed this\"),\r\n                                 \"NA\" = \"\"))\r\n\r\n#-----left join tidy_survey to melt_status-----  \r\ntidy_survey <- tidy_survey %>%\r\n  left_join(sweetness_status,\r\n            by = \"respondee\")\r\n\r\n\r\n\r\nNote: This method of converting topics into tabular variables works well when we are not dealing with too many factors (i.e. when recoding factors is not too cumbersome).\r\nExtracting a machine learning friendly dataset\r\nA reason why we might be interested in converting unstructured comment fields into structured variables is to generate data features for machine learning (i.e. predictive) purposes. For instance, we might be interested in whether there is a relationship between the topic commented on, whether the comment comes from a critic or chocolate fan, and the chocolate rating.\r\n\r\n\r\n#-----create final tidy_survey-----\r\nsurvey_rating <- survey %>%\r\n  select(respondee,\r\n         rating) # extract rating  \r\n\r\ntidy_survey <- tidy_survey %>%\r\n  select(-comment_field) %>%\r\n  left_join(survey_rating,\r\n            by = \"respondee\") %>%\r\n  mutate(respondee = str_extract(respondee, \".+(?=\\\\_[0-9]+)\"))\r\n\r\nset.seed(123) # sample reproducibly  \r\ntidy_survey %>%\r\n  sample_n(5) %>%\r\n  knitr::kable() # machine learning friendly format  \r\n\r\n\r\n\r\nDifferences between base R and stringr functions\r\nIn R, string manipulation can be performed using either base R functions or str_... functions from the stringr library. A key difference between base R and stringr functions is the order that the string and pattern are specified. The pattern, not the string, is specified first in base R functions, i.e. not in a pipe friendly order.\r\n\r\n\r\n#-----use cases for grep()-----  \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngrep(\".*\\\\bchocolate\\\\b.*\", desserts, value = F) # default is value = FALSE\r\n#> [1] 1 2 3 4 5 6  \r\n\r\nstr_which(desserts, \".*\\\\bchocolate\\\\b.*\")  \r\n#> [1] 1 2 3 4 5 6  \r\n\r\ngrep(\".*\\\\bchocolate\\\\b.*\", desserts, value = T) \r\n#> [1] \"chocolate\"            \"chocolate cake\"       \"chocolate tart\"       \"chocolate icecream\"  \r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"  \r\n\r\nstr_subset(desserts, \".*\\\\bchocolate\\\\b.*\") \r\n#> [1] \"chocolate\"            \"chocolate cake\"       \"chocolate tart\"       \"chocolate icecream\"  \r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"  \r\n\r\n# str_subset() is a wrapper around x[str_detect(x, pattern)]   \r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for grepl()-----  \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngrepl(\".*\\\\bchocolate\\\\b.*\", desserts) \r\n#> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  \r\n\r\nstr_detect(desserts, \".*\\\\bchocolate\\\\b.*\")  \r\n#> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  \r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for gsub()-----   \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngsub(\"chocolate\", \"vanilla\", desserts) \r\n#> [1] \"vanilla\"            \"vanilla cake\"       \"vanilla tart\"       \"vanilla icecream\"   \"vanilla cookies\"   \r\n#> [6] \"dark vanilla fudge\" \"fruit\"              \"fruit tart\"         \"fruit sorbet\"  \r\n\r\nstr_replace_all(desserts, \"chocolate\", \"vanilla\") \r\n#> [1] \"vanilla\"            \"vanilla cake\"       \"vanilla tart\"       \"vanilla icecream\"   \"vanilla cookies\"   \r\n#> [6] \"dark vanilla fudge\" \"fruit\"              \"fruit tart\"         \"fruit sorbet\"    \r\n\r\n\r\n\r\n\r\n\r\nbaser_vs_stringr <- microbenchmark(grep = grep(\".*\\\\bchocolate\\\\b.*\", desserts, value = F),\r\n                                   str_which = str_which(desserts, \".*\\\\bchocolate\\\\b.*\"),\r\n                                   gsub = gsub(\"chocolate\", \"vanilla\", desserts),\r\n                                   str_replace_all = str_replace_all(desserts, \"chocolate\", \"vanilla\"),\r\n                                   grepl = grepl(\".*\\\\bchocolate\\\\b.*\", desserts),\r\n                                   str_detect = str_detect(desserts, \".*\\\\bchocolate\\\\b.*\"),  \r\n                             times = 1000)\r\n\r\nautoplot(baser_vs_stringr)  \r\n\r\n\r\n\r\n\r\nNote: Base R functions are significantly faster than their stringr equivalents.\r\nOther resources\r\nTips on regular expression usage are based on the excellent regular expressions vignette from stringr.\r\nStrings chapter from R4DS by Garrett Grolemund and Hadley Wickham.\r\nThe Rstudio stringr cheatsheet.\r\nSites for testing your own regular expressions:\r\nhttps://regex101.com/\r\n\r\nMany R functions require R regex classes to be wrapped in a second set of [ ], e.g. [[:punct:]].↩\r\n",
    "preview": {},
    "last_modified": "2020-12-31T14:30:47+10:30",
    "input_file": {}
  }
]
