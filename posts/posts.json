[
  {
    "path": "posts/2021-01-30-data-table-part-1/",
    "title": "Introduction to R data.table",
    "description": "To data.table, dplyr or even to Pandas?",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "data cleaning",
      "data.table",
      "dplyr",
      "Pandas",
      "R",
      "Python"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCreate a test dataset\r\nPrinciples of data.table\r\ndata.table query structure\r\ndata.table efficiency gains\r\n\r\nFilter data\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\nBenchmark data filtering\r\n\r\nSort data\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\nBenchmark data sorting\r\n\r\nSelect data columns\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\nBenchmark column selection\r\n\r\nColumn creation\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\nBenchmark column creation\r\n\r\nSimple group by operations\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\nBenchmark simple group by operations\r\n\r\nSummary\r\nOther resources\r\n\r\nIntroduction\r\nAn inevitability of programming is that languages come and go. I am reminded of this when colleagues tell me about the first languages that they learnt or used in their academic research.\r\nIt’s taken me a while to realise that programming languages are still human constructs - they hold sway when they are utilised en mass (i.e. Python for machine learning) and user factions may emerge if two very different ways of doing the same thing concurrently exist.\r\nIn R, the latter can manifest in the form of data.table versus dplyr debates. 1\r\n\r\n\r\n\r\nBoth R packages contain a comprehensive stack of functions for data wrangling. The tidyverse dplyr approach emphasises code readability whilst data.table scales complex manipulations of large datasets very efficiently. You can compare the efficiency of data.table versus other data wrangling packages on large datasets here.\r\nWhilst I prefer to use dplyr on small datasets where data.table efficiency gains are negligible, I recommend using data.table when:\r\nYou are using very large datasets (datasets over 1 million rows) and\r\nYou need to use group by operations for data cleaning or feature engineering.\r\nLet’s explore this for ourselves.\r\n\r\n\r\n#-----load required packages-----  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,\r\n               ids, # generate random ids\r\n               tidyverse,\r\n               data.table,\r\n               lubridate, \r\n               lobstr, # trace objects in memory  \r\n               microbenchmark,\r\n               reticulate)\r\n\r\n#-----set up the Python reticulate engine-----  \r\nconda_list() # list available conda environments\r\nuse_condaenv(\"r-reticulate\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nCreate a test dataset\r\nImagine you have a dataset describing how students are engaging with online courses:\r\nEach student has a unique ID.\r\nThere are 5 different online platforms (labelled platforms A, B, C, D and E).\r\nStudents have the option of taking different courses within the same platform or switching to a different platform.\r\nStart dates are recorded when the student starts the first course in a new platform.\r\nEnd dates are also recorded when the student exits a platform.\r\nNote: The code used to create this survey can be accessed from my github repository here.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#-----examine the first few rows of data-----  \r\nstudent_courses %>%\r\n  head(6) \r\n\r\n\r\n# A tibble: 6 x 5\r\n  student_id online_platform online_course platform_start_~\r\n  <chr>      <chr>           <chr>         <date>          \r\n1 00007f23   E               fitness_trai~ 2017-05-21      \r\n2 00007f23   A               UX_design     2018-09-05      \r\n3 00007f23   E               website_desi~ 2016-06-23      \r\n4 00007f23   C               bread_baking  2018-03-03      \r\n5 00007f23   A               metal_welding 2017-11-29      \r\n6 00007f23   D               metal_welding 2016-03-09      \r\n# ... with 1 more variable: platform_end_date <date>\r\n\r\nPrinciples of data.table\r\nIn R, datasets exist as data.frame type objects. To apply data.table functions on a dataset, we need to convert a data.frame into a data.table object using setDT().\r\nThis function is flexible as it converts a data.frame by reference (i.e. without creating a duplicate data.table copy) and assigns both data.table and data.frame classes to the converted object.\r\n\r\n\r\n#-----converting a data frame into a data table-----  \r\nclass(student_courses)\r\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\nmem_used()\r\n#> 222,639,352 B\r\n\r\ntracemem(student_courses) # track object assignment in memory  \r\n#> [1] \"<0000022B07B7AB70>\"   \r\n\r\nsetDT(student_courses) # data.table is assigned to a new location in memory   \r\n#> tracemem[0x0000022b07b7ab70 -> 0x0000022b047a8478]: as.list.data.frame as.list vapply vapply_1i setDT   \r\n\r\nuntracemem(student_courses)   \r\n\r\nmem_used() # note that computer memory has not doubled following setDT()    \r\n#> 242,674,176 B  \r\n\r\nclass(student_courses)\r\n#> [1] \"data.table\" \"data.frame\"    \r\n\r\n\r\n\r\ndata.table query structure\r\nA data.table query is structured in the form DT[i, j, by] where:\r\nData selection (i.e. filtering or sorting rows) is performed in the i placeholder.\r\nData column selection or creation is performed in the j placeholder.\r\nGrouping data by variable(s) is performed in the by placeholder.\r\n\r\n\r\n\r\ndata.table efficiency gains\r\nThere are a few reasons why data.table operations are fast:\r\nMany of its operations, including grouping, reading and writing, are parallelised by default.\r\ndata.table automatically creates a secondary index (or key) of the columns used to subset data, so that subsequent operations on the same column are much faster.\r\ndata.table has a much faster order() function, which is also utilised for the evaluation of groupings.\r\nYou can use the operator := to add, delete or modify columns in place, which is a faster alternative to R’s default copy-on-modify behaviour.\r\n\r\n\r\n#-----create data.frame and data.table objects----- \r\ndf <- data.frame(id = seq(1:5),\r\n                 letter = letters[1:5])\r\n\r\ndt <- as.data.table(df)  \r\n\r\n#-----update a data.frame column using copy-on-modify----- \r\ndf_2 <- df %>%\r\n  mutate(letter = toupper(letter)) \r\n\r\n# the new data frame is a shallow copy of the original data frame \r\n# only modified columns are newly created in memory     \r\n\r\nref(df, df_2)\r\n#> o [1:0x22b03b6c998] <df[,2]> \r\n#> +-id = [2:0x22b7f3f6350] <int> \r\n#> \\-letter = [3:0x22b0c54b328] <chr> \r\n \r\n#> o [4:0x22b03b90e78] <df[,2]> \r\n#> +-id = [2:0x22b7f3f6350] \r\n#> \\-letter = [5:0x22b05f9b890] <chr> \r\n\r\n#-----update a data.table column by reference-----\r\nobj_addr(dt)\r\n#> [1] \"0x22b016ed030\"\r\n\r\ndt[, letter := toupper(letter)]\r\n\r\nobj_addr(dt)\r\n#> [1] \"0x22b016ed030\"  \r\n\r\n\r\n\r\n\r\n\r\ndt # check that dt has been modified in place    \r\n\r\n\r\n   id letter\r\n1:  1      A\r\n2:  2      B\r\n3:  3      C\r\n4:  4      D\r\n5:  5      E\r\n\r\nNote: You do not need to assign datasets to names when modifying by reference using the := operator.\r\nFilter data\r\nUsing dplyr versus data.table\r\nThe syntax for filtering data is very similar for dplyr and data.table.\r\n\r\n\r\n#-----filter student_courses using dplyr-----  \r\nstudent_courses %>%\r\n  filter(online_platform == \"A\") \r\n\r\nstudent_courses %>%\r\n  filter(online_platform != \"A\") \r\n\r\nstudent_courses %>%\r\n  filter(online_platform %in% c(\"A\", \"C\")) \r\n\r\nstudent_courses %>%\r\n  filter(student_id == \"00007f23\", # a comma represents the condition and/&\r\n        between(platform_start_date, \"2017-01-01\", \"2017-12-31\"))\r\n\r\nstudent_courses %>%\r\n  filter(str_detect(online_course, \"^R_\")) \r\n\r\n\r\n\r\nA difference is that data.table also contains a list of helper functions with optimised performance for filtering on specific data types like characters or integers.\r\n\r\n\r\n#-----filter student_courses using data.table-----  \r\nstudent_courses[online_platform == \"A\"] \r\n\r\nstudent_courses[online_platform != \"A\"] \r\n\r\nstudent_courses[online_platform %chin% c(\"A\", \"C\")] \r\n\r\nstudent_courses[student_id == \"00007f23\" & platform_start_date %between% c(\"2017-01-01\", \"2017-12-31\")]  \r\n\r\nstudent_courses[online_course %like% \"R_\"]\r\n\r\n# %chin% is equivalent to but much faster than %in%  \r\n# %like% allows you to search for a pattern in a character vector \r\n# %between% allows you to search for values in a closed interval  \r\n\r\n\r\n\r\nUsing Pandas\r\nAnd because R versus Python language wars are unnecessary, let’s filter the same dataset using the Python Pandas package and benchmark the results.\r\nNote: Objects generated in R only exist within the R environment. To share data between R <> Python environments, we need to convert R data.frames into Pandas DataFrames by calling them into the Python environment.\r\n\r\n#-----convert R data.frame object into the Python environment-----  \r\nimport pandas as pd  \r\n\r\npd_courses = r.student_courses # call R data.frame into Python environment  \r\n\r\n# convert pd_courses date columns using pd.to_datetime  \r\n\r\npd_courses[\"platform_start_date\"] = pd.to_datetime(pd_courses[\"platform_start_date\"])  \r\npd_courses[\"platform_end_date\"] = pd.to_datetime(pd_courses[\"platform_end_date\"]) \r\n\r\n\r\n#-----filter pd_courses using Pandas----- \r\npd_courses[pd_courses[\"online_platform\"] == \"A\"]  \r\npd_courses[pd_courses[\"online_platform\"] != \"A\"]   \r\npd_courses[pd_courses[\"online_platform\"].isin([\"A\", \"C\"])]   \r\npd_courses[pd_courses[\"online_course\"].str.startswith(\"R_\")]    \r\n# str.startswith() does not accept regex input i.e. \"^R_\" does not work \r\n\r\n\r\n#-----create separate conditions to filter complex multiple conditions-----  \r\nstudent = pd_courses[\"student_id\"] == \"00007f23\" \r\ndate_start = pd_courses[\"platform_start_date\"] >= \"2017-01-01\"\r\ndate_end = pd_courses[\"platform_start_date\"] <= \"2017-12-31\"\r\n\r\npd_courses[student & date_start & date_end] \r\n\r\nBenchmark data filtering\r\nYou can use the R package microbenchmark and Python package timeit to measure code performance.\r\nThe R function microbenchmark() runs each expression 100 times by default with the argument times = 100L. It outputs summary statistics for how long it takes to evaluate a single expression.\r\nThe Python function timeit() runs each expression 1,000,000 times by default and outputs in seconds the time taken to evaluate everything. We will change the latter number to 100 times with the argument number = 100 to match our R microbenchmark() input.\r\n\r\n\r\n\r\n\r\nTable 1: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nfilter(student_courses, online_platform == “A”)\r\n8.187601\r\n10.689101\r\n17.09465\r\n11.636700\r\n23.23580\r\n194.5344\r\n100\r\nstudent_courses[online_platform == “A”]\r\n7.538301\r\n8.389951\r\n10.58403\r\n9.082351\r\n10.09505\r\n29.4155\r\n100\r\nfilter(student_courses, online_platform %in% c(“A”, “C”))\r\n15.617701\r\n19.252551\r\n27.49079\r\n21.539851\r\n33.78445\r\n202.1497\r\n100\r\nstudent_courses[online_platform %chin% c(“A”, “C”)]\r\n11.982701\r\n14.004051\r\n17.32510\r\n15.068302\r\n16.51220\r\n38.9129\r\n100\r\nfilter(student_courses, str_detect(online_course, “^R_”))\r\n90.994701\r\n99.890700\r\n107.00307\r\n102.883751\r\n108.89270\r\n321.4779\r\n100\r\nstudent_courses[online_course %like% “R_”]\r\n56.770701\r\n65.711201\r\n68.57233\r\n67.439351\r\n69.43465\r\n82.5095\r\n100\r\n\r\n\r\n\r\n\r\n\r\nTable 2: Units: milliseconds\r\nexpr\r\nmean\r\nneval\r\npd_courses[pd_courses[“online_platform”] == “A”]\r\n0.000205\r\n100\r\npd_courses[pd_courses[“online_platform”].isin([“A”, “C”])]\r\n0.000071\r\n100\r\npd_courses[pd_courses[“online_course”].str.startswith(“R_”)]\r\n0.000061\r\n100\r\n\r\nSort data\r\nUsing dplyr versus data.table\r\nSorting a dataframe can be computationally expensive when you need to order your dataset by multiple variables. This is why I only sort my dataframe once, at the start of the data cleaning workflow after basic data cleaning operations have been performed.\r\n\r\n\r\n#-----sort data using dplyr-----    \r\nstudent_courses %>%\r\n  arrange(student_id) \r\n\r\nstudent_courses %>%\r\n  arrange(student_id,\r\n          online_platform,\r\n          desc(platform_start_date)) \r\n\r\n# a descending start date shows the most recent dates first  \r\n\r\n\r\n\r\nIn data.table, data sorting is also performed inside i of DT[i, j, by]. Using the operator - in front of a variable allows sorting by descending order.\r\n\r\n\r\n#-----sort student_courses using data.table----- \r\nstudent_courses[order(student_id)]  \r\n\r\nstudent_courses[order(student_id,\r\n                      online_platform,\r\n                      -platform_start_date)]  \r\n\r\n# you can also order columns in place using setorder()  \r\n\r\nsetorder(student_courses,\r\n         student_id, online_platform, -platform_start_date)  \r\n\r\n\r\n\r\nUsing Pandas\r\n\r\n#-----sort student_courses using Pandas----- \r\npd_courses.sort_values(\"student_id\")\r\npd_courses.sort_values([\"student_id\", \"online_platform\", \"platform_start_date\"],\r\n                       ascending = [True, True, False])  \r\n                       \r\n# for ascending, True sorts in ascending order and False sorts in descending order \r\n\r\nBenchmark data sorting\r\nWe can see that the order() method from data.table sorts data much faster than its equivalent dplyr function.\r\n\r\n\r\n\r\n\r\nTable 3: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\narrange(student_courses, student_id)\r\n1283.6492\r\n1307.9110\r\n1312.7568\r\n1313.4558\r\n1331.4368\r\n1332.4763\r\n10\r\nstudent_courses[order(student_id)]\r\n107.5158\r\n116.3908\r\n140.3964\r\n126.5030\r\n134.6152\r\n288.2055\r\n10\r\narrange(student_courses, student_id, online_platform, desc(platform_start_date))\r\n1268.7200\r\n1290.2830\r\n1305.1752\r\n1300.3950\r\n1318.2260\r\n1356.9325\r\n10\r\nstudent_courses[order(student_id, online_platform, -platform_start_date)]\r\n117.5911\r\n118.5124\r\n128.6181\r\n128.1828\r\n136.4534\r\n140.5916\r\n10\r\n\r\n\r\n\r\n\r\n\r\nTable 4: Units: milliseconds\r\nexpr\r\nmean\r\nneval\r\npd_courses.sort_values(“student_id”)\r\n0.000074\r\n100\r\npd_courses.sort_values([“student_id”, “online_platform”, “platform_start_date”],\r\n\r\n\r\nascending = [True, True, False])\r\n0.000073\r\n100\r\n\r\nSelect data columns\r\nUsing dplyr versus data.table\r\nIn dplyr, performing operations on a tibble will always return another data frame, unless you explicitly use pull() to extract a column as a vector.\r\n\r\n\r\n#-----select column(s) using dplyr-----   \r\nstudent_courses %>%\r\n  select(student_id)\r\n\r\nstudent_courses %>%\r\n  select(c(student_id, online_platform, online_course))\r\n\r\nstudent_courses %>%\r\n  select(contains(\"date\", ignore.case = F))\r\n\r\n#-----output a data.frame with select()-----\r\nstudent_courses %>%\r\n  select(student_id) %>%\r\n  class()\r\n#> [1] \"data.table\" \"data.frame\"  \r\n\r\n#-----output a vector with pull()-----\r\nstudent_courses %>%\r\n  pull(student_id) %>%\r\n  class()\r\n#> [1] \"character\"\r\n\r\n\r\n\r\nIn data.table, column selection is performed inside j of DT[i, j, by] and returns a data.table if you wrap column names inside a list.\r\n\r\n\r\n#-----select column(s) using data.table-----  \r\nstudent_courses[,\r\n                .(student_id)]\r\n\r\nstudent_courses[,\r\n                .(student_id,\r\n                  online_platform,\r\n                  online_course)]\r\n\r\ngrep(\"date\", names(student_courses), value = T)\r\n#> [1] \"platform_start_date\" \"platform_end_date\" \r\n\r\nstudent_courses[,\r\n                grep(\"date\", names(student_courses), value = T), \r\n                with = F]\r\n\r\n#-----output a data frame by wrapping column names inside a list-----  \r\nclass(student_courses[, .(student_id)])\r\n#> [1] \"data.table\" \"data.frame\"   \r\n\r\n#-----output a vector-----\r\nclass(student_courses[, student_id])  \r\n#> [1] \"character\"\r\n\r\nclass(student_courses[[\"student_id\"]])  \r\n#> [1] \"character\"\r\n\r\n\r\n\r\nUsing Pandas\r\nSelecting a single column will return a Pandas Series or DataFrame, depending on whether you wrap the column selection inside single versus double square brackets respectively.\r\nNote: The syntax for selecting columns using regex may be confusing to R users, as it is done by applying the .filter(regex = \"pattern\") method to the Pandas DataFrame.\r\n\r\n#-----select column(s) using using Pandas-----   \r\npd_courses[[\"student_id\"]]\r\npd_courses[[\"student_id\", \"online_platform\", \"online_course\"]]\r\npd_courses.filter(regex = \"date\")\r\n\r\n\r\n#-----output a Pandas DataFrame-----\r\ntype(pd_courses[[\"student_id\"]])\r\n#> <class 'pandas.core.frame.DataFrame'>\r\n\r\n\r\n#-----output a Pandas Series or list-----\r\ntype(pd_courses[\"student_id\"])\r\n#> <class 'pandas.core.series.Series'> \r\ntype(list(pd_courses[\"student_id\"]))\r\n#> <class 'list'>   \r\n\r\nBenchmark column selection\r\nInterestingly, benchmarking shows that dplyr performs slightly faster than data.table for column selections.\r\n\r\n\r\n\r\n\r\nTable 5: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nselect(student_courses, c(student_id, online_platform, online_course))\r\n1897.902\r\n2308.500\r\n2594.774\r\n2606.450\r\n2902.151\r\n3340.501\r\n100\r\nstudent_courses[, .(student_id, online_platform, online_course)]\r\n4726.101\r\n7540.151\r\n20363.269\r\n9157.300\r\n28884.351\r\n206756.701\r\n100\r\nselect(student_courses, contains(“date”, ignore.case = F))\r\n1210.901\r\n1539.701\r\n1836.665\r\n1805.002\r\n2077.601\r\n3324.001\r\n100\r\nstudent_courses[, grep(“date”, names(student_courses), value = T), with = F]\r\n832.400\r\n1651.351\r\n5297.077\r\n1930.450\r\n2515.851\r\n26035.501\r\n100\r\n\r\n\r\n\r\n\r\n\r\nTable 6: Units: milliseconds\r\nexpr\r\nmean\r\nneval\r\npd_courses[[“student_id”, “online_platform”, “online_course”]]\r\n0.000074\r\n100\r\npd_courses.filter(regex = “date”)\r\n0.000070\r\n100\r\n\r\nColumn creation\r\nUsing dplyr versus data.table\r\nIn dplyr version >= 1.0.0, we can use mutate() in combination with across() to create new columns or apply transformations across one or multiple columns. A shallow data.frame copy is created whenever a column is modified, which we then assign a name to.\r\n\r\n\r\n#-----create column(s) using dplyr-----  \r\n# create new columns \r\nstudent_courses <- student_courses %>%\r\n  mutate(platform_dwell_length = platform_end_date - platform_start_date,\r\n         platform_start_year = as.numeric(str_extract(platform_start_date, \"^.{4}(?!//-)\")))\r\n\r\n#-----create a new column by evaluating multiple conditions using dplyr-----  \r\nstr_subset(unique(student_courses$online_course), \"^R_\")\r\n#> [1] \"R_beginner\"     \"R_advanced\"     \"R_intermediate\"\r\nstr_subset(unique(student_courses$online_course), \"^Python_\")\r\n#> [1] \"Python_intermediate\" \"Python_advanced\"     \"Python_beginner\"    \r\n\r\nstudent_courses <- student_courses %>%\r\n  mutate(studied_programming = case_when(str_detect(online_course, \"^R_\") ~ \"Studied R\",\r\n                                         str_detect(online_course, \"^Python_\") ~ \"Studied Python\",\r\n                                         TRUE ~ \"No\"))    \r\n\r\n#-----remove newly created columns using dplyr-----  \r\nstudent_courses <- student_courses %>%\r\n  select(-c(platform_dwell_length,\r\n            platform_start_year, \r\n            studied_programming))\r\n\r\n\r\n\r\nData frame outputs are slightly different in data.table:\r\nOne or multiple transformations are modified in place using the operator :=.\r\nUse subassignment to only extract the columns transformed inside j of DT[i, j, by].\r\n\r\n\r\n#-----create column(s) using data.table-----  \r\nstudent_courses[,\r\n                c(\"platform_dwell_length\",\r\n                  \"platform_start_year\") := .(platform_end_date - platform_start_date,  \r\n                                              as.numeric(str_extract(platform_start_date, \"^.{4}(?!//-)\")))]    \r\n\r\n#-----create a new column by evaluating multiple conditions using data.table-----\r\nstudent_courses[,\r\n                studied_programming := fcase(\r\n                  str_detect(online_course, \"^R_\"), \"Studied R\",\r\n                  str_detect(online_course, \"^Python_\"), \"Studied Python\",\r\n                  default = \"No\"\r\n                )]\r\n\r\n#-----remove newly created columns using data.table-----\r\nstudent_courses[,\r\n                c(\"platform_dwell_length\",\r\n                  \"platform_start_year\",\r\n                  \"studied_programming\") := NULL]\r\n\r\n\r\n\r\n\r\n\r\n#-----only columns transformed inside j will be kept-----  \r\n# use subassignment if you want to only subset the transformed columns     \r\nplatform_records <- student_courses[,\r\n                                    .(id = toupper(student_id),\r\n                                      platform = tolower(online_platform))]  \r\n\r\nncol(platform_records)\r\n#> [1] 2  \r\n\r\n\r\n\r\nUsing Pandas\r\nIn Pandas, we can create new columns by directing applying operations onto columns or using apply() or applymap() with functions to transform single columns or element-wise across the entire data frame respectively.\r\n\r\n#-----create column(s) using Pandas-----\r\n# create new columns  \r\npd_courses[\"platform_dwell_length\"] = (pd_courses[\"platform_end_date\"] - pd_courses[\"platform_start_date\"]).dt.days  \r\npd_courses[\"platform_start_year\"] = pd_courses[\"platform_start_date\"].dt.year   \r\n\r\n\r\n#-----create a new column by evaluating multiple conditions using Pandas-----   \r\ndef clean_course_lan(value):\r\n    if value.startswith(\"R_\"):\r\n        return \"Studied R\"   \r\n    elif value.startswith(\"Python_\"):\r\n        return \"Studied Python\"\r\n    else:\r\n        return \"No\"   \r\n      \r\npd_courses[\"studied_programming\"] = pd_courses[\"online_course\"].apply(clean_course_lan)   \r\n\r\n# .apply(axis = 0) is the default argument i.e. apply function to each column     \r\n\r\n\r\n#-----remove newly created columns using Pandas----- \r\npd_courses.drop(columns = [\"platform_dwell_length\", \"platform_start_year\",\"studied_programming\"],\r\n                inplace = True)\r\n\r\nBenchmark column creation\r\n\r\n\r\n\r\n\r\nTable 7: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nmutate(student_courses, platform_dwell_length = platform_end_date - platform_start_date)\r\n4.200901\r\n7.095552\r\n31.39509\r\n7.576301\r\n9.899601\r\n225.1000\r\n100\r\nstudent_courses[, :=(“platform_dwell_length”, platform_end_date - platform_start_date)]\r\n3.216401\r\n5.070651\r\n22.52057\r\n6.159601\r\n7.606351\r\n223.2226\r\n100\r\ndplyr_case_when\r\n231.383000\r\n245.059351\r\n269.16451\r\n266.694701\r\n275.570701\r\n460.8236\r\n100\r\ndata.table_fcase\r\n192.988001\r\n203.025351\r\n206.63159\r\n206.098901\r\n208.810851\r\n238.1292\r\n100\r\n\r\n\r\n\r\n\r\n\r\nTable 8: Units: milliseconds\r\nexpr\r\nmean\r\nneval\r\n(pd_courses[“platform_end_date”] - pd_courses[“platform_start_date”]).dt.days\r\n0.000068\r\n100\r\npd_courses[“online_course”].apply(clean_course_names)\r\n0.000067\r\n100\r\n\r\nSimple group by operations\r\nUsing dplyr versus data.table\r\nEvaluating summary characteristics for subsets of data is a key area where data.table significantly outperforms dplyr. A grouping is specified using the group_by() function in dplyr and inside the by placeholder of DT[i, j, by] in data.table.\r\n\r\n\r\n#-----find total number of courses per student in dplyr-----    \r\nstudent_courses %>%\r\n  group_by(student_id) %>%\r\n  summarise(total_courses = n()) %>%\r\n  ungroup()\r\n\r\n#----find total number of distinct courses per student in dplyr-----   \r\nstudent_courses %>%\r\n  group_by(student_id) %>%\r\n  summarise(total_distinct_courses = n_distinct(online_course)) %>%\r\n  ungroup() \r\n\r\n#----find the first course studied per student and platform in dplyr-----\r\nstudent_courses %>%\r\n  group_by(student_id, online_platform) %>% # group by two variables    \r\n  filter(row_number() == 1L) %>% # return only the first row    \r\n  ungroup()\r\n\r\n\r\n\r\n\r\n\r\n#-----find total number of courses per student in data.table-----    \r\nstudent_courses[,\r\n                .(total_courses = .N),\r\n                by = student_id]   \r\n\r\n#----find total number of distinct courses per student in data.table-----   \r\nstudent_courses[,\r\n                .(total_distinct_courses = length(unique(online_course))),\r\n                by = student_id]   \r\n\r\n#----find the first course studied per student and platform in data.table-----\r\nstudent_courses[,\r\n                .SD[1L],\r\n                by = .(student_id, online_platform)]\r\n\r\n\r\n\r\nUsing Pandas\r\nWhen grouping on Pandas DataFrames, remember to reset the data frame index using .reset_index() at the end of your chain of operations.\r\n\r\n#-----find total number of courses per student in data.table-----    \r\n(pd_courses.groupby([\"student_id\"])[\"online_course\"]\r\n           .agg(\"count\")\r\n           .reset_index())   \r\n\r\n\r\n#----find total number of distinct courses per student in data.table-----   \r\n(pd_courses.groupby([\"student_id\"])[\"online_course\"]\r\n           .nunique()\r\n           .reset_index())   \r\n\r\n\r\n#----find the first course studied per student and platform in data.table-----\r\n(pd_courses.groupby([\"student_id\", \"online_platform\"])[\"online_course\"]\r\n           .agg(\"first\")\r\n           .reset_index())  \r\n\r\nBenchmark simple group by operations\r\n\r\n\r\n\r\n\r\nTable 9: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nstudent_courses %>% group_by(student_id) %>% summarise(total_courses = n())\r\n2227.1088\r\n2333.9136\r\n2425.33944\r\n2424.84395\r\n2531.0939\r\n2581.5307\r\n10\r\nstudent_courses[, .(total_courses = .N), by = student_id]\r\n51.2735\r\n52.4911\r\n55.87759\r\n55.38830\r\n58.7923\r\n62.9705\r\n10\r\nstudent_courses %>% group_by(student_id) %>% summarise(total_distinct_courses = n_distinct(online_course))\r\n3927.1009\r\n4167.0155\r\n4385.47140\r\n4344.30120\r\n4520.3397\r\n5003.1574\r\n10\r\nstudent_courses[, .(total_distinct_courses = length(unique(online_course))), by = student_id]\r\n817.3141\r\n891.6328\r\n961.38979\r\n940.12225\r\n1035.8530\r\n1119.5027\r\n10\r\nstudent_courses %>% group_by(student_id, online_platform) %>% filter(row_number() == 1L)\r\n7813.6237\r\n8053.7621\r\n8566.70527\r\n8584.17265\r\n9098.9110\r\n9508.7333\r\n10\r\nstudent_courses[, .SD[1L], by = .(student_id, online_platform)]\r\n72.8512\r\n80.5371\r\n89.63890\r\n86.30555\r\n99.3182\r\n119.6032\r\n10\r\n\r\n\r\n\r\n\r\n\r\nTable 10: Units: milliseconds\r\nexpr\r\nmean\r\nneval\r\npd_courses.groupby([“student_id”])[“online_course”].agg(“count”)\r\n0.000080\r\n100\r\npd_courses.groupby([“student_id”])[“online_course”].nunique()\r\n0.000065\r\n100\r\npd_courses.groupby([“student_id”, “online_platform”])[“online_course”].agg(“first”)\r\n0.000072\r\n100\r\n\r\nSummary\r\nMost data.table operations significantly outperform their dplyr equivalents in processing speed. I use data.table when grouping on large datasets (i.e. on datasets with greater than ~ 0.5 million rows) and use dplyr for day-to-day analyses of smaller datasets.\r\nAn interesting observation is that the Pandas package seems to perform faster than both data.table and dplyr, by even a few orders of magnitude. I will need to confirm whether this observation is real or a quirk related to differences in the execution of R microbenchmark() versus Python timeit() benchmarking code.\r\nOther resources\r\nA stack overflow discussion about the best use cases for data.table versus dplyr.\r\nA great side-by-side comparison of data.table versus dplyr functions from a blog post by Atrebas.\r\nA list of advanced data.table operations and tricks from a blog post by Andrew Brooks.\r\nAn explanation of how data.table modifies by reference from a blog post by Tyson Barrett.\r\n\r\nI am extremely thankful for these Twitter debates, as they first drew my attention to data.table.↩︎\r\n",
    "preview": "posts/2021-01-30-data-table-part-1/finalplot.png",
    "last_modified": "2021-02-13T23:31:50+11:00",
    "input_file": {},
    "preview_width": 1125,
    "preview_height": 726
  },
  {
    "path": "posts/2021-01-02-volcano-plots-with-ggplot2/",
    "title": "Volcano plots with ggplot2",
    "description": "Revising my grammar of graphics.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "data visualisation",
      "ggplot2",
      "tidyverse",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nImport a test dataset\r\nCreate a simple volcano plot\r\nAdd horizontal and vertical plot lines\r\nModify the x-axis and y-axis\r\nAdd colour, size and transparency\r\nLayer subplots\r\nLabel points of interest\r\nModify legend label positions\r\nModify plot labels and theme\r\nAnnotate text\r\nOther resources\r\n\r\nIntroduction\r\nIn 2018, whilst still an R newbie, I participated in the RLadies Melbourne community lightning talks and talked about how to visualise volcano plots in R. Volcano plots are probably an obscure concept outside of bioinformatics, but their construction nicely showcases the elegance of ggplot2.\r\nIn the last two years, a number of small and handy functions have been added to dplyr and ggplot2, which this post has been updated to reflect. 1\r\nLet’s get started then.\r\n\r\n\r\n#----load required packages----  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,  \r\n               tidyverse, \r\n               janitor, # for cleaning names  \r\n               scales, # for scale transformations  \r\n               ggrepel) # for optimal label separation  \r\n\r\n\r\n\r\nImport a test dataset\r\nWe start with a dataset with four columns:\r\nEntrez ID - stores the unique gene ID.\r\nGene symbol - stores the gene name associated with an unique Entrez ID.\r\nFold change - stores the change in gene expression level detected in diseased versus healthy tissue.\r\nAdjusted P-value - stores the P-value adjusted with a false discovery rate (FDR) correction for multiple testing.\r\nEvery row represents a unique gene expression fold change, which fulfills tidy data requirements for creating data visualisations.\r\nNote: The data used originates from Fu et al. Nat Cell Biol. 2015 and a copy of the original dataset can be found here.\r\n\r\n\r\n#----import and clean dataset---- \r\ndiseased_vs_healthy <- read.delim(here(\"data\", \"limma-voom_luminalpregnant-luminallactate.txt\"),\r\n                                  header = TRUE,\r\n                                  sep = \"\\t\")  \r\n\r\ndiseased_vs_healthy <- janitor::clean_names(diseased_vs_healthy)  \r\n\r\ndiseased_vs_healthy <- diseased_vs_healthy %>%\r\n  mutate(fold_change = 2^log_fc) %>%\r\n  select(entrezid,\r\n         symbol,\r\n         fold_change,\r\n         adj_p_val)  \r\n\r\n\r\n\r\n\r\nentrezid\r\nsymbol\r\nfold_change\r\nadj_p_val\r\n14367\r\nFzd5\r\n2.0716416\r\n0.0542520\r\n244144\r\nUsp35\r\n0.4197075\r\n0.0598292\r\n100216534\r\nSnord96a\r\n1.2373480\r\n0.9999999\r\n66151\r\nPrr13\r\n1.4296409\r\n0.9999999\r\n229445\r\nCtso\r\n1.0506197\r\n0.9999999\r\n\r\nCreate a simple volcano plot\r\nA basic version of a volcano plot depicts:\r\nAlong its x-axis: log2(fold_change)\r\nAlong its y-axis: -log10(adj_p_val)\r\nNote: The y-axis depicts -log10(adj_p_val), which allows the points on the plot to project upwards as the fold change greatly increases or decreases. This is more intuitive to visualise, as we are interested in the data points at the edges of the ‘volcano spray’.\r\nThe versatility of ggplot2 also means that you don’t need to store data transformations as separate variables for plotting. You can apply transformations directly inside ggplot(data, aes(x, y)) or alternatively by using scale_x_continuous(trans = \"...\") or coord_trans(x, y).\r\n\r\n\r\n#----create a simple volcano plot----\r\nvol_plot <- diseased_vs_healthy %>%\r\n  ggplot(aes(x = log2(fold_change),\r\n             y = -log10(adj_p_val))) + \r\n  geom_point() \r\n\r\nvol_plot # a simple volcano plot is created\r\n\r\n\r\n\r\n\r\nNote: For single layer plots, use %>% pipes with ggplot2 functions for convenience and readability.\r\nAdd horizontal and vertical plot lines\r\nThe functions geom_hline() and geom_vline() can be used to add extra horizontal and vertical lines on your plot respectively. In our example, I am interested in constructing boundaries for genes which have adj_p_val <= 0.05 and fold_change <= 0.5 or fold_change >= 2.\r\n\r\n\r\n#----plot extra quandrants----\r\nvol_plot + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\")   \r\n\r\n\r\n\r\n\r\nModify the x-axis and y-axis\r\nVolcano plots should have a symmetrical x-axis. One way we can do this is by manually setting the limits of the x-axis using xlim(min, max).\r\n\r\n\r\n#----identify the best values for xlim----\r\ndiseased_vs_healthy %>%\r\n  pull(fold_change) %>%\r\n  min() %>%\r\n  log2() %>%\r\n  floor() \r\n#> [1] -10   \r\n\r\ndiseased_vs_healthy %>%\r\n  pull(fold_change) %>%\r\n  max() %>%\r\n  log2() %>%\r\n  ceiling()\r\n#> [1] 8  \r\n\r\nmax(abs(-10), abs(8))\r\n#> [1] 10  \r\n\r\n#----add xlim----  \r\nvol_plot + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") + \r\n  xlim(-10, 10) # manually specify x-axis limits \r\n\r\n\r\n\r\n\r\nWe can also change the limits of the x-axis inside scale_x_continuous. This method also gives us the flexibility to finetune the spacing and labelling of axis tick marks.\r\n\r\n\r\n#----modify scale_x_continuous----\r\nvol_plot + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)), # modify x-axis tick intervals    \r\n                     limits = c(-10, 10)) \r\n\r\n\r\n\r\n\r\nNote: The value specified inside the argument scale_continuous_x(limits = ...) supercedes the range of values specified inside the argument scale_continuous_x(breaks = ...).\r\nAdd colour, size and transparency\r\nTo visualise different groups of genes using different colours, point sizes, shapes or transparencies, we need to categorise genes into different groups and store these categories as a new parameter i.e. new column of data.\r\nI am interested in labelling genes into the following groups:\r\nGenes with a fold change >= 2 and adjusted p-value <= 0.05 labelled as ‘up’.\r\nGenes with a fold change <= 0.5 and adjusted p-value <= 0.05 labelled as ‘down’.\r\nAll other genes labelled as ‘ns’ i.e. non-significant.\r\n\r\n\r\n#----create a new column which distinguishes individual rows by type---- \r\ndiseased_vs_healthy <- diseased_vs_healthy %>%\r\n  mutate(gene_type = case_when(fold_change >= 2 & adj_p_val <= 0.05 ~ \"up\",\r\n                               fold_change <= 0.5 & adj_p_val <= 0.05 ~ \"down\",\r\n                               TRUE ~ \"ns\"))   \r\n\r\n#----obtain a summary of gene_type numbers----           \r\ndiseased_vs_healthy %>%\r\n  count(gene_type)\r\n\r\n# the function count() is equivalent to     \r\n\r\n# diseased_vs_healthy %>%\r\n#   group_by(gene_type) %>%\r\n#   summarize(count = n()) \r\n\r\n\r\n\r\n\r\ngene_type\r\nn\r\ndown\r\n1245\r\nns\r\n13578\r\nup\r\n981\r\n\r\nIn ggplot2, we have the option to visualise different groups by point colour, size, shape and transparency by modifying parameter like scale_color_manual() etc. A tidy way of doing this is to store manual specifications as vectors.\r\n\r\n\r\n#----double check gene_type categories----    \r\ndiseased_vs_healthy %>%\r\n  distinct(gene_type) %>%\r\n  pull()  \r\n#> [1] \"down\" \"up\"   \"ns\"    \r\n\r\n\r\n\r\n\r\n\r\n#----add colour, size and alpha (transparency) specs to volcano plot---- \r\ncols <- c(\"up\" = \"#ffad73\", \"down\" = \"#26b3ff\", \"ns\" = \"grey\") \r\nsizes <- c(\"up\" = 2, \"down\" = 2, \"ns\" = 1) \r\nalphas <- c(\"up\" = 1, \"down\" = 1, \"ns\" = 0.5)\r\n\r\ndiseased_vs_healthy %>%\r\n  ggplot(aes(x = log2(fold_change),\r\n             y = -log10(adj_p_val),\r\n             fill = gene_type,    \r\n             size = gene_type,\r\n             alpha = gene_type)) + \r\n  geom_point(shape = 21, # specify shape and colour as fixed local parameters    \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  scale_fill_manual(values = cols) + # modify colour\r\n  scale_size_manual(values = sizes) + # modify point size\r\n  scale_alpha_manual(values = alphas) + # modify point transparency\r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)), # modify x-axis tick intervals    \r\n                     limits = c(-10, 10))  \r\n\r\n\r\n\r\n\r\nLayer subplots\r\nWe can also overlay subplots on top of our main plot. This is useful when we want to highlight a subset of our data using different colours, shapes and etc. When overlaying plots, we should not use %>% pipes but use global ggplot(data = “…”) and local geom_point(data = ...) arguments instead.\r\n\r\n\r\n#----add a subplot to the main volcano plot----  \r\nils <- str_subset(diseased_vs_healthy$symbol, \"^[I|i]l[0-9]+$\")  \r\n\r\nil_genes <- diseased_vs_healthy %>%\r\n  filter(symbol %in% ils) \r\n\r\nggplot(data = diseased_vs_healthy, # original data  \r\n       aes(x = log2(fold_change), y = -log10(adj_p_val))) + \r\n  geom_point(colour = \"grey\", alpha = 0.5) +\r\n  geom_point(data = il_genes, # data subset    \r\n             size = 2,\r\n             shape = 21,\r\n             fill = \"firebrick\",\r\n             colour = \"black\")     \r\n\r\n\r\n\r\n\r\nNote: Unless local aesthetics are specified, secondary geom_point() functions will still inherit glocal ggplot aesthetics.\r\nLabel points of interest\r\nWe can also label a subset of data using geom_text(), geom_label(), geom_text_repel() or geom_label_repel and by specifying which column to display as text using the local argument geom_text(aes(label = ...)).\r\nNote: adjusting the parameters for optimal text separation using geom_text_repel can be a bit fiddly. I generally start by modifying force and then deciding which region of the plot I want to nudge my text or labels towards. See here for more tips on adjusting geom_text_repel parameters.\r\n\r\n\r\n#-----create subplots of interest---- \r\nsig_il_genes <- diseased_vs_healthy %>%\r\n  filter(symbol %in% c(\"Il15\", \"Il34\", \"Il24\"))\r\n\r\nup_il_genes <- diseased_vs_healthy %>%\r\n  filter(symbol == \"Il24\")\r\n\r\ndown_il_genes <- diseased_vs_healthy %>%\r\n  filter(symbol %in% c(\"Il15\", \"Il34\"))\r\n\r\n#----visual layered and labelled volcano plot---- \r\nggplot(data = diseased_vs_healthy,\r\n       aes(x = log2(fold_change),\r\n           y = -log10(adj_p_val))) + \r\n  geom_point(aes(colour = gene_type), \r\n             alpha = 0.2, \r\n             shape = 16,\r\n             size = 1) + \r\n  geom_point(data = up_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"firebrick\", \r\n             colour = \"black\") + \r\n  geom_point(data = down_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"steelblue\", \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  geom_label_repel(data = sig_il_genes, # add labels last as the top layer to plot   \r\n                   aes(label = symbol),\r\n                   force = 2,\r\n                   nudge_y = 1) +\r\n  scale_colour_manual(values = cols) + \r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),     \r\n                     limits = c(-10, 10))  \r\n\r\n\r\n\r\n\r\nModify legend label positions\r\nIf we want to change the order of discrete figure legend labels, you need to modify the factor levels of your dataset grouping. This can be done using the forcats package, which allows us to easily modify factor levels.\r\n\r\n\r\n#----modify factor gene_type levels----  \r\ndiseased_vs_healthy <- diseased_vs_healthy %>%\r\n  mutate(gene_type = fct_relevel(gene_type, \"up\", \"down\")) \r\n\r\n# apply fct_relevel so the first two levels are \"up\" and \"down\"  \r\n\r\n#----recreate volcano plot----  \r\nggplot(data = diseased_vs_healthy,\r\n       aes(x = log2(fold_change),\r\n           y = -log10(adj_p_val))) + \r\n  geom_point(aes(colour = gene_type), \r\n             alpha = 0.2, \r\n             shape = 16,\r\n             size = 1) + \r\n  geom_point(data = up_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"firebrick\", \r\n             colour = \"black\") + \r\n  geom_point(data = down_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"steelblue\", \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  geom_label_repel(data = sig_il_genes, # add labels last as the top layer to plot   \r\n                   aes(label = symbol),\r\n                   force = 2,\r\n                   nudge_y = 1) +\r\n  scale_colour_manual(values = cols) + \r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),     \r\n                     limits = c(-10, 10))   \r\n\r\n\r\n\r\n\r\nNote: If we wanted to change the text displayed in the figure legend, we would need to modify the factor levels (i.e. variable categories) themselves.\r\nModify plot labels and theme\r\nThe last finishing touches include modifying plot labels and plot theme.\r\nThe function labs() is a handy way of specifying all your plot labels within a single function. You can also assign labels as NULL to prevent them from being displayed.\r\nA plot can be further improved by changing its theme() and/or by modifying individual theme() parameters.\r\n\r\n\r\n#----add plot labels and modify plot theme----\r\nfinal_plot <- ggplot(data = diseased_vs_healthy,\r\n       aes(x = log2(fold_change),\r\n           y = -log10(adj_p_val))) + \r\n  geom_point(aes(colour = gene_type), \r\n             alpha = 0.2, \r\n             shape = 16,\r\n             size = 1) + \r\n  geom_point(data = up_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"firebrick\", \r\n             colour = \"black\") + \r\n  geom_point(data = down_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"steelblue\", \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  geom_label_repel(data = sig_il_genes, # add labels last as the top layer to plot   \r\n                   aes(label = symbol),\r\n                   force = 2,\r\n                   nudge_y = 1) +\r\n  scale_colour_manual(values = cols) + \r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),     \r\n                     limits = c(-10, 10)) +\r\n  labs(title = \"Gene expression changes in diseased versus healthy samples\",\r\n       x = \"log2(fold change)\",\r\n       y = \"-log10(adjusted P-value)\",\r\n       colour = \"Expression \\nchange\") +\r\n  theme_bw() + # creates a white background\r\n  theme(panel.border = element_rect(colour = \"black\", fill = NA, size= 0.5), # creates a plot border \r\n        panel.grid.minor = element_blank(),\r\n        panel.grid.major = element_blank()) \r\n\r\nfinal_plot \r\n\r\n\r\n\r\n\r\nNote: You can specify panel.grid... = element_line(linetype = \"dotted\") inside theme() to create dotted gridlines along the x and/or y axis. Major gridline positions are inherited from the values of axis breaks.\r\nAnnotate text\r\nWe can add descriptive labels to our plot by using the function annotate() to display text.\r\n\r\n\r\n#----annotate text inside plot----\r\nfinal_plot + \r\n  annotate(\"text\", x = 7, y = 10,\r\n           label = \"3 interleukins of interest\", color = \"firebrick\")\r\n\r\n\r\n\r\n\r\nOther resources\r\nThe excellent and interactive code-along RStudio Cloud ggplot2 tutorials\r\nRStudio ggplot cheatsheet\r\nSTHDA tutorial on ggplot2 axis transformations\r\n\r\nThe original coding logic should still be attributed to Chuanxin Liu, my former PhD student. I also recommend the excellent RStudio Cloud ggplot2 tutorials, which have taught me a few new tricks.↩︎\r\n",
    "preview": "posts/2021-01-02-volcano-plots-with-ggplot2/finalplot.png",
    "last_modified": "2021-01-30T23:23:49+11:00",
    "input_file": {},
    "preview_width": 2187,
    "preview_height": 1350
  },
  {
    "path": "posts/2020-12-31-cleaning-free-text-and-wrangling-strings/",
    "title": "Cleaning free text and wrangling strings",
    "description": "These are some common data cleaning things.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "data cleaning",
      "regex",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCreate a test dataset\r\nIntroduction to regular expressions\r\nMatch characters\r\nCharacter anchors\r\nCharacter classes and groupings\r\nGreedy versus lazy matches\r\nLook arounds\r\n\r\nImprove comment field readability\r\nManually extract topics of interest\r\nExtract a machine learning friendly dataset\r\nDifferences between base R and stringr functions\r\nOther resources\r\n\r\nIntroduction\r\nComment fields sit somewhere in between tidy tabular data entries and large text files (i.e. documents) in terms of wrangling effort. They require human naunce to decode and the quality and completeness of comments vary between individual entries.\r\nThis makes it hard to gauge whether cleaning comment fields is a worthwhile endeavour (especially when you have multiple other data sources that need examining). Luckily, some knowledge of string manipulations and regular expressions can help simplify this process.\r\nLet’s get started.\r\n\r\n\r\n#-----load required packages-----  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,  \r\n               tidyverse,  \r\n               microbenchmark) \r\n\r\n\r\n\r\nCreate a test dataset\r\nLet’s imagine that my local chocolate company, Haighs Chocolates, wants to understand what food critics versus Haighs fans think about their newest product. They send out a bag of free samples with a link to an online survey that asks individuals to rate their chocolates (on a scale of 1 to 10) and provide additional comments.\r\nNote: The code used to create this survey can be accessed from my github repository here.\r\n\r\n\r\n\r\n\r\n\r\n#-----examine the first few rows of data-----  \r\nsurvey %>%\r\n  head(6)  \r\n\r\n\r\n# A tibble: 6 x 3\r\n  respondee rating comment_field                                      \r\n  <chr>     <chr>  <chr>                                              \r\n1 expert_1  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n2 expert_2  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n3 expert_3  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n4 expert_4  10     \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n5 expert_5  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n6 fan_1     9      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> Delicious ~\r\n\r\nOh dear, it looks like we will first need to use regular expressions to remove all the html tags embedded within survey$comment_field.\r\nIntroduction to regular expressions\r\nRegular expressions, or regex, can be thought of as a separate syntax for handling patterns in strings. In R, regular expressions can be directly enclosed inside quotes like character strings or explicitly referenced inside regex(). For convenience, I prefer the former approach but the latter approach can help increase code readability.\r\n\r\n\r\n#-----call regular expressions in R-----\r\nmany_apples <- c(\"Apple\", \"apple\", \"APPLE\", \"apples\")\r\n\r\nstr_extract(many_apples, \r\n            \"apples?\")  \r\n#> [1] NA       \"apple\"  NA       \"apples\"\r\n\r\n#-----call regular expressions in R using regex()-----\r\n# regex() provides additional arguments\r\n\r\nstr_extract(many_apples, \r\n            regex(\"apples?\", ignore_case = T))  \r\n#> [1] \"Apple\"  \"apple\"  \"APPLE\"  \"apples\"\r\n\r\n# regex() also allows comments to improve regex readability  \r\n\r\nstr_extract(many_apples, \r\n            regex(\"\r\n                  apple  # contains the word apple\r\n                  s?  # contains zero or one of the letter s\r\n                  \" , comments = T))\r\n#> [1] NA       \"apple\"  NA       \"apples\"  \r\n\r\n\r\n\r\nMatch characters\r\nSome sequences of characters have specific meanings. For example, s refers to the letter \"s\" but \\s refers to any type of white space. To call whitespace in R, a second backslash \\ is required to escape special character behaviour i.e. \\\\s.\r\n\r\n\r\n#-----examples of special character sequences-----  \r\nwords_and_spaces <- c(\" a cat\",\r\n                      \"acat\",\r\n                      \"a   cat\",\r\n                      \"a\\ncat\",\r\n                      \"a\\\\ncat\")\r\n\r\n# \"a\\\\s+cat\" calls variations of a...cat separated by one or more whitespaces \r\n# note that the string \"a\\ncat\" also counts because \\n refers to a new line\r\n\r\nstr_extract(words_and_spaces, \"a\\\\s+cat\")  \r\n#> [1] \"a cat\"   NA        \"a   cat\" \"a\\ncat\"  NA      \r\n\r\n# \"\\\\S+\" refers to everything that is not white space (starting from left to right)  \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"\r\n\r\n\r\n\r\nNote: The special characters \\s versus \\S allow the extraction of opposite pattern types. In another example, lowercase \\w refers to any word character whilst uppercase \\W and lowercase [^\\w] both refer to anything that is not a word character.\r\nCharacter anchors\r\nI feel that the goal of writing good regex is to be as specific as possible. This is why character anchors are useful (i.e. using ^ and $ to denote the start and end of your string respectively).\r\nIf we revisit the example above, we can see that the presence or absence of character anchors produces very different outputs.\r\n\r\n\r\n#-----impact of character anchors-----    \r\nwords_and_spaces <- c(\" a cat\",\r\n                      \"acat\",\r\n                      \"a   cat\",\r\n                      \"a\\ncat\",\r\n                      \"a\\\\ncat\")\r\n\r\n# \"\\\\S+\" refers to everything that is not white space (from left to right unless specified)  \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"  \r\n\r\nstr_extract(words_and_spaces, \"^\\\\S+\")  \r\n#> [1] NA       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"   \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+$\") \r\n#> [1] \"cat\"     \"acat\"    \"cat\"     \"cat\"     \"a\\\\ncat\"       \r\n\r\n\r\n\r\nCharacter classes and groupings\r\nCharacter classes and groupings are handy for extracting specific letter and/or digit combinations. Some special characters found inside character classes and groupings are:\r\nThe operation or is represented by | i.e [a|c]\r\nThe operation range is represented by - i.e. [a-z]\r\nThe operation excludes is represented by ^ i.e. [^a-c]\r\nNote: Representation of a single character is denoted by [] and representation of a grouping i.e. combination of characters is denoted by ().\r\n\r\n\r\n#-----extract patterns using character classes i.e. []-----    \r\nstrange_fruits <- c(\"apple1\",\r\n                    \"bapple2\",\r\n                    \"capple3\",\r\n                    \"dapple4\",\r\n                    \"epple5\",\r\n                    \"aggle0\")\r\n\r\nstr_extract(strange_fruits, \"[a-d]\")\r\n#> [1] \"a\" \"b\" \"c\" \"d\" NA  \"a\"  \r\n\r\nstr_extract(strange_fruits, \"[a-d][^p]\")\r\n#> [1] NA   \"ba\" \"ca\" \"da\" NA   \"ag\"   \r\n\r\n# [a-d][^p] refers to one character between a and d followed by one character that is not p  \r\n\r\nstr_extract(strange_fruits, \"[0|4-9]\")\r\n#> [1] NA  NA  NA  \"4\" \"5\" \"0\"   \r\n\r\n# [0|4-9] refers to one number that is zero or a number from 4 to 9    \r\n\r\n\r\n\r\n\r\n\r\n#-----extract character using groupings i.e. ()-----     \r\nstrange_fruits <- c(\"apple1\",\r\n                    \"bapple2\",\r\n                    \"capple3\",\r\n                    \"dapple4\",\r\n                    \"epple5\",\r\n                    \"aggle1\")  \r\n\r\nstr_extract(strange_fruits, \"a(pp|gg)le\")\r\n#> [1] \"apple\" \"apple\" \"apple\" \"apple\" NA      \"aggle\"    \r\n\r\n# groups can be referenced by their order of appearance i.e. \\\\1 = first group  \r\n\r\nstr_extract(strange_fruits, \"(a)(p|g)\\\\2\")\r\n#> [1] \"app\" \"app\" \"app\" \"app\" NA    \"agg\"   \r\n\r\n# (a) is group 1 and can be called using \\\\1    \r\n# (p|g) is group 2 and can be called using \\\\2     \r\n\r\n\r\n\r\nGreedy versus lazy matches\r\nIn R, regular expression parsing is non-greedy by default. This means that we need to add quantifiers * and + to greedily extract zero or more and one or more characters respectively.\r\nIn contrast, using a non-greedy match allows you to extract just the first characters before a white space or punctuation mark. This is useful for trimming strings or extracting file or object names.\r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for greedy matches-----   \r\nmessy_dates <- c(\"Thursday 24th May\",\r\n                 \"Thursday  24th May  \",\r\n                 \" May\",\r\n                 \"May    \")\r\n\r\nstr_extract(messy_dates, \"^\\\\w\")      \r\n#> [1] \"T\" \"T\" NA  \"M\"   \r\n\r\n# greedily extract the first word in the string    \r\n\r\nstr_extract(messy_dates, \"^\\\\w+\")   \r\n#> [1] \"Thursday\" \"Thursday\" NA      \"May\"   \r\n\r\nstr_extract(messy_dates, \"^\\\\w{1,}\") # the quantifier + and {1,} are equivalent    \r\n#> [1] \"Thursday\" \"Thursday\" NA      \"May\"    \r\n\r\nstr_extract(messy_dates, \"^(\\\\S+)\")  \r\n#> [1] \"Thursday\" \"Thursday\" NA      \"May\"    \r\n\r\n#-----use cases for non-greedy matches----- \r\nstr_replace_all(messy_dates, \"\\\\s\" , \"-\") # replaces each individual whitespace\r\n#> [1] \"Thursday-24th-May\"    \"Thursday--24th-May--\" \"-May\"                  \"May----\"       \r\n\r\nstr_replace_all(messy_dates, \"\\\\s{1,2}\" , \"-\") \r\n#> [1] \"Thursday-24th-May\"  \"Thursday-24th-May-\" \"-May\"                \"May--\"         \r\n\r\n# use look arounds (next topic) to replace the whitespace(s) after the first word     \r\n\r\nstr_replace_all(messy_dates, \"(?<=^\\\\w{1,100})\\\\s{1,2}\" , \"-\") \r\n#> [1] \"Thursday-24th May\"   \"Thursday-24th May  \" \" May\"                 \"May-  \"     \r\n\r\n\r\n\r\nNote: For further details explaining the regex syntax for the last example, read this stack overflow post.\r\nLook arounds\r\nLook around operations are useful when you are unsure of the pattern itself, but you know exactly what its preceding or following pattern is. I’ve found that the clearest explanation of look around operations comes from the RStudio cheetsheet on string_r, as depicted below.\r\n\r\n\r\n\r\nFigure 1: Taken from the RStudio stringr cheatsheet\r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for different types of look arounds-----  \r\nrecipes <- c(\"crossiant recipes\",\r\n             \"apple pie recipe\",\r\n             \"chocolate cake  recipe\", # extra space\r\n             \"cookie receipe\",  # deliberate typo\r\n             \"secret KFC-recipe\", \r\n             \"very secret  McDonalds soft-serve recipe\") # extra space  \r\n\r\n# use positive look-ahead (?=...) to extract the preceding word\r\n\r\nstr_extract(recipes, \"\\\\S+(?=\\\\s*recipes?)\")   \r\n#> [1] \"crossiant\"  \"pie\"        \"cake\"       NA           \"KFC-\"       \"soft-serve\"   \r\n\r\n# use positive look-behind (?<=) on \"secret\" to identify the secret recipes  \r\n\r\nstr_extract(recipes, \"(?<=secret\\\\s{1,10})\\\\S+.+\")   \r\n#> [1] NA                            NA                            NA                           \r\n#> [4] NA                            \"KFC-recipe\"                  \"McDonalds soft-serve recipe\"   \r\n\r\n\r\n\r\nNote: Positive look-behinds require defined boundary specifications i.e. the operation + needs to be converted into {1,1000}.\r\nImprove comment field readability\r\nWith regex revised, let us return to our Haighs chocolate survey. The first thing we can see is that html tags have been retained inside the comment field and that this field is very long (i.e. difficult to read).\r\nWe can improve the readability of the survey by:\r\nRemoving all html tags using regex.\r\nSeparating phrases into columns using separate().\r\n\r\n\r\n#-----re-examine survey data-----\r\nsurvey %>%\r\n  head(5)   \r\n\r\n\r\n# A tibble: 5 x 3\r\n  respondee rating comment_field                                      \r\n  <chr>     <chr>  <chr>                                              \r\n1 expert_1  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n2 expert_2  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n3 expert_3  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n4 expert_4  10     \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n5 expert_5  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n\r\n#-----remove html tags-----\r\nremove_html_tags <- regex(\"\r\n                          <  # starts with <\r\n                          [^>]+  # contains one or more of all characters excepting > \r\n                          >  # ends with >\r\n                          \", comments = T)\r\n\r\nremove_more_html <- regex(\"\r\n                          \\\\& # starts with &\r\n                          \\\\w+ # contains one or more word characters\r\n                          \\\\; # ends with ;\r\n                          \", comments = T) \r\n\r\nsurvey <- survey %>%\r\n  mutate(comment_field = str_replace_all(comment_field, remove_html_tags, \"\"),\r\n         comment_field = str_replace_all(comment_field, remove_more_html, \"\"))\r\n\r\n#-----examine comment field-----  \r\nsurvey %>%\r\n  select(comment_field) %>%\r\n  head(5) \r\n\r\n\r\n# A tibble: 5 x 1\r\n  comment_field                                                       \r\n  <chr>                                                               \r\n1 \" Grade A beans. Easily melts. Smooth chocolate shell, with a crunc~\r\n2 \" Grade A beans with subtle caramel hints. Melts well. Smooth exter~\r\n3 \" Grade a beans.  Caramel and vanilla undertones complement the bit~\r\n4 \" Grade A cocoa beans. Melts easily. Smooth dark chocolate contrast~\r\n5 \" Grade A beans, likely of Ecuador origin. Smooth dark chocolate co~\r\n\r\nWe can then split the single long comment field into multiple smaller columns. 1\r\n\r\n\r\n#-----separate comment field into an unknown number of columns-----    \r\nnmax <- max(str_count(survey$comment_field, \"[[:punct:]]|and|with|against\")) + 1\r\n\r\nsurvey <- survey %>%   \r\n  separate(comment_field,\r\n           into = paste0(\"Field\", seq_len(nmax)),\r\n           sep = \"[[:punct:]]|and|with|against\", # separate on punctuation or conjunctions  \r\n           remove = F,\r\n           extra = \"warn\",\r\n           fill = \"right\") \r\n\r\n#-----examine comment fields-----  \r\nsurvey %>%\r\n  select(starts_with(\"Field\")) %>%\r\n  head(5) \r\n\r\n\r\n\r\nManually extract topics of interest\r\nAfter separating the comment field into smaller fields, we see references to:\r\ncocoa bean grade\r\npresence of caramel or vanilla flavour\r\nchocolate smoothness\r\nhow well the chocolate melts\r\nsugar content/ sweetness level\r\nmalt filling\r\nchocolate coating\r\nInformation about cocoa bean grade is highly structured. This means that extracting the letter following the word “Grade” is sufficient. A similar logic can be applied to extract whether caramel or vanilla flavour or chocolate smoothness was mentioned.\r\n\r\n\r\n#----extract information about cocoa bean grade, flavour and smoothness----\r\ntidy_survey <- survey %>%\r\n  select(respondee,\r\n         comment_field) %>% \r\n  mutate(cocoa_grade = str_extract(comment_field, \"(?<=[G|g]rade\\\\s{0,10})[A-C|a-c]\"),\r\n         is_caramel = case_when(str_detect(comment_field, \"[C|c]aramel\") ~ \"yes\",\r\n                                TRUE ~ \"NA\"), \r\n         is_vanilla = case_when(str_detect(comment_field, \"[V|v]anilla\") ~ \"yes\",\r\n                                TRUE ~ \"NA\"),\r\n         is_smooth = case_when(str_detect(comment_field, \"[S|s]mooth\") ~ \"yes\",\r\n                               TRUE ~ \"NA\")) \r\n\r\n# we cannot assign TRUE ~ NA inside case_when  \r\n\r\ntidy_survey <- tidy_survey %>%\r\n  mutate_at(vars(cocoa_grade), ~ replace_na(., \"NA\"))\r\n\r\n# replace NA in cocoa_grade with the character \"NA\" for consistency  \r\n\r\n\r\n\r\nFor more descriptive fields such as whether the chocolate melts, I find it easier to first extract a matrix of fields.\r\n\r\n\r\n#----extract information about chocolate texture----\r\nmelt_matrix <- survey %>%\r\n  select_at(vars(respondee,\r\n                 starts_with(\"Field\"))) %>% \r\n  mutate_at(vars(starts_with(\"Field\")),\r\n            ~ replace(.x, !(str_detect(.x, \".*\\\\b[M|m]elt.*\\\\b.*\")), NA)) \r\n\r\n# convert fields which do not contain \"melt\" into NA and unite all fields     \r\n\r\nmelt_cols <- str_which(colnames(melt_matrix), \"^Field.+\")\r\n\r\nmelt_status <- melt_matrix %>%\r\n  unite(\"is_melty\", # new column \r\n        all_of(melt_cols), # unite these columns  \r\n        sep = \"\",\r\n        remove = T,\r\n        na.rm = T) # make sure to remove NAs  \r\n\r\n#----convert responses into factors and recode factor levels----  \r\nmelt_status$is_melty <- factor(melt_status$is_melty)\r\n\r\nlevels(melt_status$is_melty) \r\n#> [1] \"\"                     \" Easily melts\"        \" Melts easily\"        \" melts in your mouth\" \" Melts well\"         \r\n\r\nmelt_status <- melt_status %>%\r\n  mutate(is_melty = fct_collapse(is_melty,\r\n                                 \"yes\" = c(\" Easily melts\",\r\n                                           \" Melts well\",\r\n                                           \" Melts easily\",\r\n                                           \" melts in your mouth\"),\r\n                                 \"NA\" = \"\"))\r\n\r\n#----left join tidy_survey to melt_status----  \r\ntidy_survey <- tidy_survey %>%\r\n  left_join(melt_status,\r\n            by = \"respondee\")\r\n\r\n\r\n\r\nThis process is repeated for chocolate sweetness. 2\r\n\r\n\r\n#----extract information about chocolate sweetness----  \r\nsweetness_matrix <- survey %>%\r\n  select_at(vars(respondee,\r\n                 starts_with(\"Field\"))) %>% \r\n  mutate_at(vars(starts_with(\"Field\")),\r\n            ~ replace(.x, !(str_detect(.x, \".*\\\\b[S|s](weet)|(ugar).*\\\\b.*\")), NA)) \r\n\r\n# convert fields which do not contain \"sweet\" or \"sugar\" into NA and unite all fields     \r\n\r\nsweetness_cols <- str_which(colnames(sweetness_matrix), \"^Field.+\")\r\n\r\nsweetness_status <- sweetness_matrix %>%\r\n  unite(\"is_sweet\", \r\n        all_of(sweetness_cols), \r\n        sep = \"\",\r\n        remove = T,\r\n        na.rm = T) # make sure to remove NAs  \r\n\r\n#----convert responses into factors and recode factor levels---- \r\nsweetness_status$is_sweet <- factor(sweetness_status$is_sweet)\r\n\r\nlevels(sweetness_status$is_sweet)\r\n#> [1] \"\"                                      \" low sugar content \"                   \" not so sweet  I enjoyed this\"        \r\n#> [4] \"filled core may be too sweet for some\"\r\n\r\nsweetness_status <- sweetness_status %>%\r\n  mutate(is_sweet = fct_collapse(is_sweet,\r\n                                 \"yes\" = c(\"filled core may be too sweet for some\"),\r\n                                 \"no\" = c(\" low sugar content \",\r\n                                          \" not so sweet  I enjoyed this\"),\r\n                                 \"NA\" = \"\"))\r\n\r\n#----left join tidy_survey to melt_status----  \r\ntidy_survey <- tidy_survey %>%\r\n  left_join(sweetness_status,\r\n            by = \"respondee\")\r\n\r\n\r\n\r\nNote: This method of converting topics into tabular variables works well when we are not dealing with too many factors (i.e. when recoding factors is not too cumbersome).\r\nExtract a machine learning friendly dataset\r\nA reason why we might be interested in converting unstructured comment fields into structured variables is to generate data features for machine learning. For instance, we might be interested in whether there is a relationship between survey topics, whether the comment comes from a critic or chocolate fan, and the chocolate rating.\r\n\r\n\r\n#----create final tidy_survey----\r\nsurvey_rating <- survey %>%\r\n  select(respondee,\r\n         rating) # extract rating  \r\n\r\ntidy_survey <- tidy_survey %>%\r\n  select(-comment_field) %>%\r\n  left_join(survey_rating,\r\n            by = \"respondee\") %>%\r\n  mutate(respondee = str_extract(respondee, \".+(?=\\\\_[0-9]+)\"))\r\n\r\nset.seed(123) # sample reproducibly  \r\ntidy_survey %>%\r\n  sample_n(5)   \r\n\r\n\r\n\r\nDifferences between base R and stringr functions\r\nIn R, string manipulation can be performed using either base R functions or functions from the stringr library. A key difference between base R and stringr functions is the order that the string and pattern are specified. The pattern, not the string, is specified first inside base R functions, which is not a pipe friendly argument order.\r\n\r\n\r\n#-----use cases for grep()-----  \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngrep(\".*\\\\bchocolate\\\\b.*\", desserts, value = F) # default is value = FALSE\r\n#> [1] 1 2 3 4 5 6  \r\n\r\n# grep, value = FALSE only extracts the position of matching elements in the vector  \r\n\r\nstr_which(desserts, \".*\\\\bchocolate\\\\b.*\")  \r\n#> [1] 1 2 3 4 5 6  \r\n\r\ngrep(\".*\\\\bchocolate\\\\b.*\", desserts, value = T) \r\n#> [1] \"chocolate\"            \"chocolate cake\"       \"chocolate tart\"       \"chocolate icecream\"  \r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"   \r\n\r\n# grep, value = TRUE extracts the matching elements in the vector  \r\n\r\nstr_subset(desserts, \".*\\\\bchocolate\\\\b.*\") \r\n#> [1] \"chocolate\"            \"chocolate cake\"       \"chocolate tart\"       \"chocolate icecream\"  \r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"  \r\n\r\n# str_subset() is a wrapper around x[str_detect(x, pattern)]   \r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for grepl()-----  \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngrepl(\".*\\\\bchocolate\\\\b.*\", desserts) \r\n#> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  \r\n\r\nstr_detect(desserts, \".*\\\\bchocolate\\\\b.*\")  \r\n#> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE \r\n\r\ndesserts[str_detect(desserts, \".*\\\\bchocolate\\\\b.*\")]\r\n#> [1] \"chocolate\"            \"chocolate cake\"       \"chocolate tart\"       \"chocolate icecream\"   \"chocolate cookies\"    \"dark chocolate fudge\"  \r\n\r\n\r\n\r\n\r\n\r\n#-----use cases for gsub()-----   \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngsub(\"(dark )?chocolate\", \"vanilla\", desserts) \r\n#> [1] \"vanilla\"          \"vanilla cake\"     \"vanilla tart\"     \"vanilla icecream\" \"vanilla cookies\"  \"vanilla fudge\"    \"fruit\"           \r\n#> [8] \"fruit tart\"       \"fruit sorbet\"    \r\n\r\nstr_replace_all(desserts, \"(dark )?chocolate\", \"vanilla\") \r\n#> [1] \"vanilla\"          \"vanilla cake\"     \"vanilla tart\"     \"vanilla icecream\" \"vanilla cookies\"  \"vanilla fudge\"    \"fruit\"           \r\n#> [8] \"fruit tart\"       \"fruit sorbet\"            \r\n\r\n\r\n\r\n\r\n\r\nbaser_vs_stringr <- microbenchmark(grep = grep(\".*\\\\bchocolate\\\\b.*\", desserts, value = F),\r\n                                   str_which = str_which(desserts, \".*\\\\bchocolate\\\\b.*\"),\r\n                                   gsub = gsub(\"chocolate\", \"vanilla\", desserts),\r\n                                   str_replace_all = str_replace_all(desserts, \"chocolate\", \"vanilla\"),\r\n                                   grepl = grepl(\".*\\\\bchocolate\\\\b.*\", desserts),\r\n                                   str_detect = str_detect(desserts, \".*\\\\bchocolate\\\\b.*\"),  \r\n                                   times = 1000)\r\n\r\nautoplot(baser_vs_stringr)  \r\n\r\n\r\n\r\n\r\nNote: Base R functions are significantly faster than their stringr equivalents.\r\nOther resources\r\nTips on regular expression usage are based on the excellent regular expressions vignette from stringr\r\nStrings chapter from R4DS by Garrett Grolemund and Hadley Wickham\r\nRStudio stringr cheatsheet\r\nhttps://regex101.com/ - a website for testing regular expressions\r\n\r\nMany R functions require R regex classes to be wrapped in a second set of [ ], e.g. [[:punct:]].↩︎\r\nAs a repetitive step within the workflow, we might want to rewrite the generic part of this cleaning step as a function stored in a separate R script for maximal readability.↩︎\r\n",
    "preview": "posts/2020-12-31-cleaning-free-text-and-wrangling-strings/benchmark.png",
    "last_modified": "2021-02-14T11:44:18+11:00",
    "input_file": "wrangling-strings-and-cleaning-free-text.utf8.md",
    "preview_width": 1949,
    "preview_height": 1200
  }
]
