[
  {
    "path": "posts/2021-02-16-data-table-part-2/",
    "title": "Advanced data.table operations",
    "description": "Things get querysome and querysome.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2021-02-16",
    "categories": [
      "data cleaning",
      "data.table",
      "dplyr",
      "Pandas",
      "R",
      "Python"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nBasic operation sequence\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\n\r\nGroup by and summarise data\r\nUsing dplyr versus data.table\r\nUsing Pandas\r\nGroup by and lead or lag operations\r\n\r\nOther resources\r\n\r\nIntroduction\r\nThis post is a continuation of my previous comparison of R data.table versus dplyr versus Python Pandas operations.\r\nNote: The code used to create this dataset can be accessed from my github repository here.\r\n\r\n\r\n#-----load required packages-----  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,\r\n               ids, # generate random ids\r\n               tidyverse,\r\n               data.table,\r\n               microbenchmark,\r\n               reticulate)\r\n\r\n#-----set up the Python reticulate engine-----  \r\nconda_list() # list available conda environments\r\nuse_condaenv(\"r-reticulate\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe will be using the same dataset describing how students are engaging with online courses:\r\nEach student has a unique ID.\r\nThere are 5 different online platforms (labelled platforms A, B, C, D and E).\r\nStudents have the option of taking different courses within the same platform or switching to a different platform.\r\nStart dates are recorded when the student starts the first course in a new platform.\r\nEnd dates are also recorded when the student exits a platform.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#-----examine the first few rows of data-----  \r\nstudent_courses %>%\r\n  head(6)   \r\n\r\n\r\n# A tibble: 6 x 5\r\n  student_id online_platform online_course platform_start_~\r\n  <chr>      <chr>           <chr>         <date>          \r\n1 00007f23   E               fitness_trai~ 2017-05-21      \r\n2 00007f23   A               UX_design     2018-09-05      \r\n3 00007f23   E               website_desi~ 2016-06-23      \r\n4 00007f23   C               bread_baking  2018-03-03      \r\n5 00007f23   A               metal_welding 2017-11-29      \r\n6 00007f23   D               metal_welding 2016-03-09      \r\n# ... with 1 more variable: platform_end_date <date>\r\n\r\n#-----convert data frame to data.table-----  \r\nsetDT(student_courses)\r\n\r\n\r\n\r\nBasic operation sequence\r\nUsing dplyr versus data.table\r\nImagine you would like to subset all fitness training courses from platform C and E and create a column to denote that these were discounted courses. In dplyr, you can write this as a single sequence of steps and use the %>% pipe to separate each individual operation.\r\n\r\n\r\n#-----filter and append a column using dplyr-----    \r\ndply_query_1 <- student_courses %>%\r\n  filter(online_course == \"fitness_training\",\r\n         online_platform %in% c(\"C\", \"E\")) %>%\r\n  mutate(percentage_discount = 5)\r\n\r\n#-----preview the first 4 rows-----      \r\ndply_query_1 %>% \r\n  select(-contains(\"platform_\")) %>% \r\n  head(4) \r\n\r\n\r\n   student_id online_platform    online_course percentage_discount\r\n1:   00007f23               E fitness_training                   5\r\n2:   0007ca33               E fitness_training                   5\r\n3:   000899ab               E fitness_training                   5\r\n4:   0011ed00               C fitness_training                   5\r\n\r\nIn data.table, placing the filter and column transformation operations in separate or a single step produces different outputs. 1\r\n\r\n\r\n#-----filter and append a column using data.table-----  \r\n# keep filter and column transformation steps separate  \r\ndt_query_1 <- student_courses[(online_course == \"fitness_training\") \r\n                              & (online_platform %chin% c(\"C\", \"E\"))] %>%\r\n  .[, percentage_discount := 5]   \r\n\r\n#-----preview the first 4 rows-----  \r\n# subset columns which begin with \"platform_\"\r\ndate_cols <- grep(\"^platform_\", colnames(student_courses), value = T)\r\n\r\ndt_query_1[1:4, !..date_cols]\r\n\r\n\r\n   student_id online_platform    online_course percentage_discount\r\n1:   00007f23               E fitness_training                   5\r\n2:   0007ca33               E fitness_training                   5\r\n3:   000899ab               E fitness_training                   5\r\n4:   0011ed00               C fitness_training                   5\r\n\r\n\r\n\r\n#-----the wrong way to filter and append a column using data.table-----  \r\n# all rows are retained and only rows which meet the selection criteria are transformed \r\n# student_courses is also modified in reference \r\ndt_query_1_wrong <- student_courses[(online_course == \"fitness_training\") \r\n                                    & (online_platform %chin% c(\"C\", \"E\")), \r\n                                    percentage_discount := 5]\r\n\r\n#-----preview the first 4 rows-----  \r\ndt_query_1_wrong[1:4, !..date_cols]  \r\n\r\n\r\n\r\n\r\n\r\n\r\nUsing Pandas\r\nIn Pandas, chaining is currently limited to .method() operations, so we need to write separate lines of code to extract our dataset of interest.\r\n\r\nimport pandas as pd    \r\npd_courses = r.student_courses  \r\n\r\n#-----filter and append a column using Pandas-----\r\npd_query_1 = (pd_courses.loc[(pd_courses[\"online_course\"] == \"fitness_training\") &\r\n                             (pd_courses[\"online_platform\"].isin([\"C\", \"E\"]))]\r\n              .copy())\r\npd_query_1[\"percentage_discount\"] = 5\r\n\r\n\r\n#-----preview the first 4 rows-----  \r\n# subset columns which begin with \"platform_\"  \r\ndate_cols = pd_query_1.columns[pd_query_1.columns.str.contains(\"platform_\")]\r\n\r\n(pd_query_1.drop(columns = date_cols)\r\n .head(4)\r\n .reset_index(drop = True))\r\n  student_id online_platform     online_course  percentage_discount\r\n0   00007f23               E  fitness_training                    5\r\n1   0007ca33               E  fitness_training                    5\r\n2   000899ab               E  fitness_training                    5\r\n3   0011ed00               C  fitness_training                    5\r\n\r\nGroup by and summarise data\r\nA common operation is to group by variable(s) to retrieve summary data from each grouping(s).\r\nUsing dplyr versus data.table\r\nImagine that we are interested in the total number of days that each student spent on any online platform. We might obtain this by grouping on individual student IDs and calculating the total of days spent on any platform.\r\n\r\n\r\n#-----calculate platform dwell length using dplyr-----  \r\nstudent_courses <- student_courses %>%\r\n  mutate(platform_dwell_length = platform_end_date - platform_start_date,\r\n         platform_dwell_length = as.integer(platform_dwell_length))\r\n\r\n#-----calculate platform dwell length using data.table-----\r\nstudent_courses[, platform_dwell_length := platform_end_date - platform_start_date] %>%\r\n  .[, platform_dwell_length := as.integer(platform_dwell_length)]  \r\n\r\n\r\n\r\nNote: In dplyr, we need to close our sequence of operations with the function ungroup(), which removes object metadata on row groupings.\r\n\r\n\r\n#-----group by student_id and summarise platform dwell length using dplyr-----\r\ndply_query_2 <- student_courses %>%\r\n  group_by(student_id) %>%\r\n  summarise(total_dwell = sum(platform_dwell_length),\r\n            min_dwell = min(platform_dwell_length),\r\n            median_dwell = median(platform_dwell_length),  \r\n            max_dwell = max(platform_dwell_length)) %>%\r\n  ungroup()   \r\n\r\n#-----preview the first 4 rows-----   \r\ndply_query_2 %>%\r\n  head(4)\r\n\r\n\r\n\r\nIn data.table, we can choose which variable(s) to group by using either by or keyby in the by placeholder of DT[i, j, by]. The additional effect of using keyby is that keyby also orders the results and creates a secondary key for faster subsetting.\r\n\r\n\r\n#-----group by student_id and summarise total platform dwell length using data.table-----\r\ndt_query_2 <- student_courses[,\r\n                              .(total_dwell = sum(platform_dwell_length),\r\n                                min_dwell = min(platform_dwell_length),\r\n                                median_dwell = median(platform_dwell_length),\r\n                                max_dwell = max(platform_dwell_length)),\r\n                              by = student_id]\r\n\r\n#-----preview the first 4 rows-----  \r\ndt_query_2[1:4]  \r\n\r\n\r\n\r\nThe problem with this approach is that some students switch between courses on the same platform so the platform dwell length can be duplicated if we simply group by students IDs. To identify when this scenario occurs, we need to:\r\nConcatenate the online platform and platform start date to create a new column.\r\nIdentify students where length(unique(new_col)) > n().\r\n\r\n\r\n#-----create platform_key using dplyr----- \r\nstudent_courses <- student_courses %>%\r\n  mutate(platform_key = str_c(online_platform, platform_start_date, sep = \"-\"))\r\n\r\n#-----create platform_key using data.table-----  \r\nstudent_courses[,\r\n                platform_key := do.call(paste0, c(.SD, sep = \"-\")),\r\n                .SDcols = c(\"online_platform\", \"platform_start_date\")]\r\n\r\n\r\n\r\nNote: The base R function do.call() constructs and executes a function call from a name or a function and a list of arguments to be passed to it.\r\n\r\n\r\n#-----identify students with duplicate records in dplyr-----\r\ndply_query_3 <- student_courses %>%\r\n  group_by(student_id) %>%\r\n  summarise(unique_platform_signups = n_distinct(platform_key),\r\n            total_course_signups = n()) \r\n\r\n#-----return student_ids as vector and preview the first 3 elements-----\r\ndply_query_3 %>%\r\n  filter(total_course_signups > unique_platform_signups) %>%\r\n  pull(student_id) %>%\r\n  .[1:3]\r\n\r\n#-----examine duplicate records example-----\r\nstudent_courses %>%\r\n  select(-c(platform_key)) %>%\r\n  filter(student_id == \"00040980\")\r\n\r\n\r\n\r\n\r\n\r\n#-----identify students with duplicate records in data.table-----  \r\ndt_query_3 <- student_courses[,\r\n                              .(unique_platform_signups = length(unique(platform_key)),\r\n                                total_course_signups = .N),\r\n                              by = student_id] \r\n\r\n#-----return student_ids as vector and preview the first 3 elements-----\r\ndt_query_3[total_course_signups > unique_platform_signups, \r\n           student_id] %>%\r\n  .[1:3]\r\n\r\n#-----examine duplicate records example-----  \r\nstudent_courses[student_id == \"00040980\",\r\n                !\"platform_key\"]  \r\n\r\n\r\n\r\nUsing Pandas\r\nGroup by and lead or lag operations\r\nOther resources\r\nThe data analysis and visualisation in Python for ecologists Software Carpentry workshop on indexing, slicing and subsetting DataFrames in Python.\r\nA great side-by-side comparison of data.table versus dplyr functions from a blog post by Atrebas.\r\nA list of advanced data.table operations and tricks from a blog post by Andrew Brooks.\r\n\r\nPlacing the two operations in separate steps is equivalent to the dplyr approach above.↩︎\r\n",
    "preview": "posts/2021-02-16-data-table-part-2/finalplot.png",
    "last_modified": "2021-02-21T22:29:25+11:00",
    "input_file": {},
    "preview_width": 2216,
    "preview_height": 1419
  },
  {
    "path": "posts/2021-01-30-data-table-part-1/",
    "title": "Introduction to data.table",
    "description": "To data.table or dplyr? That is the question.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "data cleaning",
      "data.table",
      "dplyr",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCreate a test dataset\r\nPrinciples of data.table\r\ndata.table query structure\r\ndata.table efficiency gains\r\n\r\nFilter data\r\nUsing dplyr versus data.table\r\nBenchmark data filtering\r\n\r\nSort data\r\nUsing dplyr versus data.table\r\nBenchmark data sorting\r\n\r\nSelect data columns\r\nUsing dplyr versus data.table\r\nBenchmark column selection\r\n\r\nColumn creation\r\nUsing dplyr versus data.table\r\nBenchmark column creation\r\n\r\nSimple group by operations\r\nUsing dplyr versus data.table\r\nBenchmark simple group by operations\r\n\r\nSummary\r\nOther resources\r\n\r\nIntroduction\r\nProgramming languages are still human constructs. They hold sway when they are utilised en mass, like Python for machine learning, and user factions may emerge if very different ways of doing the same thing concurrently exist.\r\nIn R, this can manifest in the form of data.table versus dplyr debates. 1\r\n\r\n\r\n\r\nBoth R packages contain a comprehensive stack of functions for data wrangling. The tidyverse dplyr approach emphasises code readability whilst data.table scales complex manipulations of large datasets very efficiently. You can compare the efficiency of data.table versus other data wrangling packages on large datasets here.\r\nWhilst I prefer to use dplyr on small datasets where data.table efficiency gains are negligible, I recommend using data.table when:\r\nYou are using very large datasets (datasets over 0.5 million rows) and\r\nYou need to use group by operations for data cleaning or feature engineering.\r\nLet’s explore this for ourselves.\r\n\r\n\r\n# Load required packages -------------------------------------------------------  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,\r\n               lobstr, # Trace objects in memory \r\n               ids, # Generate random ids\r\n               DT, # Create interactive tables  \r\n               tidyverse,\r\n               data.table,\r\n               microbenchmark)\r\n\r\n\r\n\r\nCreate a test dataset\r\nImagine you have a dataset describing how students are engaging with online courses:\r\nEach student has a unique ID.\r\nThere are 5 different online platforms, labelled A, B, C, D and E.\r\nStudents have the option of taking different courses within the same platform or switching to a different platform.\r\nPlatform start dates are recorded when the student starts the first course on a new platform.\r\nPlatform end dates are recorded when the student exits a platform.\r\nCourse start dates are recorded when the student starts a course.\r\nCourse end dates are recorded when the student exits a course.\r\nNote: The code used to create this dataset can be accessed from my github repository here.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe can interactively examine the first 20 rows of the dataset using the R package DT.\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"00007f23\",\"00007f23\",\"00007f23\",\"00007f23\",\"00007f23\",\"00007f23\",\"000080c8\",\"000080c8\",\"0000ba9c\",\"0000ba9c\",\"0000ba9c\",\"0000ba9c\"],[\"D\",\"E\",\"E\",\"A\",\"C\",\"A\",\"D\",\"B\",\"E\",\"D\",\"D\",\"C\"],[\"metal_welding\",\"website_design\",\"fitness_training\",\"metal_welding\",\"bread_baking\",\"UX_design\",\"contemporary_dance\",\"fitness_training\",\"website_design\",\"R_beginner\",\"fitness_training\",\"website_design\"],[\"2016-03-09\",\"2016-06-23\",\"2017-05-21\",\"2017-11-29\",\"2018-03-03\",\"2018-09-05\",\"2016-10-31\",\"2018-12-04\",\"2016-10-07\",\"2017-08-08\",\"2018-02-01\",\"2018-09-22\"],[\"2016-04-20\",\"2016-08-01\",\"2017-07-17\",\"2018-01-07\",\"2018-04-22\",\"2018-11-11\",\"2016-11-09\",\"2019-01-25\",\"2016-11-30\",\"2017-09-21\",\"2018-03-29\",\"2018-10-08\"],[\"2016-03-09\",\"2016-06-23\",\"2017-05-21\",\"2017-11-29\",\"2018-03-03\",\"2018-09-05\",\"2016-10-31\",\"2018-12-04\",\"2016-10-07\",\"2017-08-08\",\"2018-02-01\",\"2018-09-22\"],[\"2016-04-20\",\"2016-08-01\",\"2017-07-17\",\"2018-01-07\",\"2018-04-22\",\"2018-11-11\",\"2016-11-09\",\"2019-01-25\",\"2016-11-30\",\"2017-09-21\",\"2018-03-29\",\"2018-10-08\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>id<\\/th>\\n      <th>platform<\\/th>\\n      <th>course<\\/th>\\n      <th>platform_start_date<\\/th>\\n      <th>platform_end_date<\\/th>\\n      <th>course_start_date<\\/th>\\n      <th>course_end_date<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":6,\"dom\":\"tip\",\"initComplete\":\"function(settings, json) {\\n$(this.api().table().header()).css({'background-color': '#37ACA1', 'color': '#fff'});\\n}\",\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[6,10,25,50,100]}},\"evals\":[\"options.initComplete\"],\"jsHooks\":[]}\r\nPrinciples of data.table\r\nIn R, datasets exist as data.frame type objects. To apply data.table functions on a dataset, we need to convert a data.frame into a data.table object using setDT().\r\nThis function is flexible as it converts a data.frame by reference (i.e. without creating a duplicate data.table copy) and assigns both data.table and data.frame classes to the converted object.\r\n\r\n\r\n# Convert data frame into data.table -------------------------------------------  \r\nclass(student_courses)\r\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\nmem_used()\r\n#> 222,639,352 B\r\n\r\n# Track object assignment in memory  \r\ntracemem(student_courses)   \r\n#> [1] \"<0000022B07B7AB70>\"   \r\n\r\nsetDT(student_courses) # data.table is assigned to a new location in memory   \r\n#> tracemem[0x0000022b07b7ab70 -> 0x0000022b047a8478]: as.list.data.frame as.list vapply vapply_1i setDT   \r\n\r\nuntracemem(student_courses)   \r\n\r\nmem_used()     \r\n#> 242,674,176 B  \r\n\r\n# Note that computer memory has not doubled following setDT()  \r\n\r\nclass(student_courses)\r\n#> [1] \"data.table\" \"data.frame\"    \r\n\r\n\r\n\r\ndata.table query structure\r\nA data.table query is structured in the form DT[i, j, by] where:\r\nData selection (i.e. filtering or sorting rows) is performed in the i placeholder.\r\nData column selection or creation is performed in the j placeholder.\r\nGrouping data by variable(s) is performed in the by placeholder.\r\n\r\n\r\n\r\ndata.table efficiency gains\r\nThere are a few reasons why data.table operations are fast:\r\nMany of its operations, including grouping, reading and writing, are parallelised by default.\r\ndata.table automatically creates a secondary index (or key) of the columns used to subset data, so that subsequent operations on the same column are much faster.\r\ndata.table has a much faster order() function, which is also utilised for the evaluation of groupings.\r\nYou can use the operator := to add, delete or modify columns in place, which is a faster alternative to R’s default copy-on-modify behaviour.\r\n\r\n\r\n# Create data.frame and data.table objects ------------------------------------- \r\ndf <- data.frame(id = seq(1:5),\r\n                 letter = letters[1:5])\r\n\r\ndt <- as.data.table(df)  \r\n\r\n# Update data.frame column using copy-on-modify -------------------------------- \r\ndf_2 <- df %>%\r\n  mutate(letter = toupper(letter)) \r\n\r\n# The new data frame is a shallow copy of the original data frame as\r\n# only modified columns are newly created in memory.       \r\n\r\nref(df, df_2)\r\n#> o [1:0x22b03b6c998] <df[,2]> \r\n#> +-id = [2:0x22b7f3f6350] <int> \r\n#> \\-letter = [3:0x22b0c54b328] <chr> \r\n \r\n#> o [4:0x22b03b90e78] <df[,2]> \r\n#> +-id = [2:0x22b7f3f6350] \r\n#> \\-letter = [5:0x22b05f9b890] <chr> \r\n\r\n# Update data.table column by reference ----------------------------------------\r\nobj_addr(dt)\r\n#> [1] \"0x22b016ed030\"\r\n\r\ndt[, letter := toupper(letter)]\r\n\r\nobj_addr(dt)\r\n#> [1] \"0x22b016ed030\"  \r\n\r\n\r\n\r\n\r\n\r\ndt # Check that dt has been modified in place      \r\n\r\n\r\n   id letter\r\n1:  1      A\r\n2:  2      B\r\n3:  3      C\r\n4:  4      D\r\n5:  5      E\r\n\r\nNote: You do not need to assign datasets to names when modifying by reference using the := operator.\r\nFilter data\r\nUsing dplyr versus data.table\r\nThe syntax for filtering data is very similar for dplyr and data.table.\r\n\r\n\r\n# Filter student_courses using dplyr -------------------------------------------    \r\n# Filter by platform A \r\nstudent_courses %>%\r\n  filter(platform == \"A\")  \r\n\r\n# Filter by all platforms excepting A  \r\nstudent_courses %>%\r\n  filter(platform != \"A\") \r\n\r\n# Filter by platforms A and C    \r\nstudent_courses %>%\r\n  filter(platform %in% c(\"A\", \"C\")) \r\n\r\n# Using a comma is a substitute for the condition 'and' \r\nstudent_courses %>%\r\n  filter(id == \"00007f23\", \r\n        between(platform_start_date, \"2017-01-01\", \"2017-12-31\"))\r\n\r\n# Filter by a variable using regex   \r\nstudent_courses %>%\r\n  filter(str_detect(course, \"^R_\")) \r\n\r\n\r\n\r\nA difference is that data.table also contains a list of helper functions with optimised performance for filtering on specific data types like characters or integers.\r\n\r\n\r\n# Filter student_courses using data.table --------------------------------------  \r\n# Filter by platform A \r\nstudent_courses[platform == \"A\"] \r\n\r\n# Filter by all platforms excepting A   \r\nstudent_courses[platform != \"A\"] \r\n\r\n# Operator %chin% is equivalent to but much faster than %in%  \r\nstudent_courses[platform %chin% c(\"A\", \"C\")] \r\n\r\n# Operator %between% allows you to search for values in a closed inclusive interval \r\nstudent_courses[id == \"00007f23\" & platform_start_date %between% c(\"2017-01-01\", \"2017-12-31\")]  \r\n\r\n# Operator %like% allows you to search for a pattern in a character vector \r\nstudent_courses[course %like% \"R_\"]  \r\n\r\n\r\n\r\nBenchmark data filtering\r\nYou can use the R package microbenchmark to measure code performance. 2\r\nThe function microbenchmark() runs each expression 100 times by default with the argument times = 100L. It outputs summary statistics on how long it takes to evaluate a single expression.\r\n\r\nTable 1: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nfilter(student_courses, platform == “A”)\r\n8.6185\r\n9.85425\r\n14.02690\r\n10.63355\r\n12.59240\r\n32.5160\r\n100\r\nstudent_courses[platform == “A”]\r\n8.1147\r\n8.68425\r\n11.79464\r\n9.23450\r\n10.76145\r\n29.5605\r\n100\r\nfilter(student_courses, platform %in% c(“A”, “C”))\r\n13.6701\r\n14.83205\r\n20.73142\r\n16.04955\r\n18.52705\r\n133.6671\r\n100\r\nstudent_courses[platform %chin% c(“A”, “C”)]\r\n12.5168\r\n13.84470\r\n19.00417\r\n14.96820\r\n16.58040\r\n126.5837\r\n100\r\nfilter(student_courses, str_detect(course, “^R_”))\r\n88.7389\r\n97.34130\r\n104.40473\r\n102.87980\r\n107.93375\r\n142.4213\r\n100\r\nstudent_courses[course %like% “R_”]\r\n59.2113\r\n66.55935\r\n71.34264\r\n70.67060\r\n73.52060\r\n96.0989\r\n100\r\n\r\nSort data\r\nUsing dplyr versus data.table\r\nSorting a data frame can be computationally expensive when multiple variables need to be ranked. This is why I recommending sorting your dataset once, right after basic data cleaning operations have been performed.\r\n\r\n\r\n# Sort student_courses using dplyr ---------------------------------------------   \r\nstudent_courses %>%\r\n  arrange(course_start_date) \r\n\r\nstudent_courses %>%\r\n  arrange(platform,\r\n          id, \r\n          desc(platform_start_date)) \r\n\r\n# Sorting by a descending date ranks the most recent date as first  \r\n\r\n\r\n\r\nIn data.table, sorting is also performed inside i of DT[i, j, by]. Using the operator - in front of a variable allows sorting by descending order.\r\n\r\n\r\n# Sort student_courses using data.table ---------------------------------------- \r\nstudent_courses[order(course_start_date)]  \r\n\r\nstudent_courses[order(platform,\r\n                      id,\r\n                      -platform_start_date)]  \r\n\r\n# You can also order columns in place using setorder()\r\nsetorder(student_courses,\r\n         id,\r\n         platform_start_date,\r\n         course_start_date)  \r\n\r\n\r\n\r\nBenchmark data sorting\r\nYou can see that order() from data.table sorts data much faster than its equivalent dplyr function.\r\n\r\nTable 2: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\narrange(student_courses, course_start_date)\r\n27.2534\r\n35.2252\r\n44.51070\r\n49.7276\r\n52.7478\r\n61.6616\r\n25\r\nstudent_courses[order(course_start_date)]\r\n54.8829\r\n62.0147\r\n70.32617\r\n69.7907\r\n72.0628\r\n96.8317\r\n25\r\narrange(student_courses, platform, id, desc(platform_start_date))\r\n5605.0036\r\n5654.6456\r\n5702.51392\r\n5682.9645\r\n5712.8049\r\n6182.9437\r\n25\r\nstudent_courses[order(platform, id, -platform_start_date)]\r\n119.1436\r\n125.0282\r\n134.31395\r\n130.4740\r\n144.8021\r\n160.8236\r\n25\r\n\r\nSelect data columns\r\nUsing dplyr versus data.table\r\nIn dplyr, performing operations on a tibble will always return another data frame, unless you explicitly use pull() to extract a column as a vector.\r\n\r\n\r\n# Select column(s) using dplyr -------------------------------------------------   \r\nstudent_courses %>%\r\n  select(id)\r\n\r\nstudent_courses %>%\r\n  select(c(id, platform, course))\r\n\r\n# Select columns(s) using regex  \r\nstudent_courses %>%\r\n  select(contains(\"date\", ignore.case = F))\r\n\r\n# Output data.frame with select() ----------------------------------------------\r\nstudent_courses %>%\r\n  select(id) %>%\r\n  class()\r\n#> [1] \"data.table\" \"data.frame\"  \r\n\r\n# Output vector with pull() ----------------------------------------------------\r\nstudent_courses %>%\r\n  pull(id) %>%\r\n  class()\r\n#> [1] \"character\"\r\n\r\n\r\n\r\nIn data.table, column selection is performed inside j of DT[i, j, by] and returns a data.table if you wrap column names inside a list.\r\n\r\n\r\n# Select column(s) using data.table --------------------------------------------  \r\nstudent_courses[,\r\n                .(id)]\r\n\r\nstudent_courses[,\r\n                .(id,\r\n                  platform,\r\n                  course)]\r\n\r\n# Select column(s) using regex  \r\ngrep(\"date\", names(student_courses), value = T)\r\n#> [1] \"platform_start_date\" \"platform_end_date\" \r\n\r\nstudent_courses[,\r\n                grep(\"date\", names(student_courses), value = T), \r\n                with = F]\r\n\r\n# Output data frame by wrapping column names inside a list ---------------------  \r\nclass(student_courses[, .(id)])\r\n#> [1] \"data.table\" \"data.frame\"   \r\n\r\n# Output vector with [[x]] -----------------------------------------------------\r\nclass(student_courses[, id])  \r\n#> [1] \"character\"\r\n\r\nclass(student_courses[[\"id\"]])  \r\n#> [1] \"character\"\r\n\r\n\r\n\r\nBenchmark column selection\r\nInterestingly, the benchmark below shows that dplyr is slightly faster than data.table for column selections.\r\n\r\nTable 3: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nselect(student_courses, c(id, platform, course))\r\n1.8944\r\n2.32380\r\n2.633437\r\n2.48395\r\n2.82525\r\n5.3393\r\n100\r\nstudent_courses[, .(id, platform, course)]\r\n4.3514\r\n5.12335\r\n14.512512\r\n7.25425\r\n24.44105\r\n136.2222\r\n100\r\nselect(student_courses, contains(“date”, ignore.case = F))\r\n1.2884\r\n1.55195\r\n1.789318\r\n1.72585\r\n1.97540\r\n3.0404\r\n100\r\nstudent_courses[, grep(“date”, names(student_courses), value = T), with = F]\r\n1.3837\r\n1.75865\r\n7.824120\r\n2.72795\r\n3.90910\r\n128.7633\r\n100\r\n\r\nColumn creation\r\nUsing dplyr versus data.table\r\nIn dplyr version >= 1.0.0, you can use mutate() in combination with across() to create new columns or apply transformations across one or multiple columns.\r\nA shallow data.frame copy is created whenever a column is modified, which you then assign a name to.\r\n\r\n\r\n# Create column(s) using dplyr -------------------------------------------------  \r\n# Create new columns from existing variables   \r\nstudent_courses <- student_courses %>%\r\n  mutate(platform_dwell_length = platform_end_date - platform_start_date,\r\n         platform_start_year = as.numeric(str_extract(platform_start_date, \"^.{4}(?!//-)\")))\r\n\r\n# Create column(s) with multiple conditions using dplyr ------------------------  \r\nstr_subset(unique(student_courses$course), \"^R_\")\r\n#> [1] \"R_beginner\"     \"R_advanced\"     \"R_intermediate\"\r\nstr_subset(unique(student_courses$course), \"^Python_\")\r\n#> [1] \"Python_intermediate\" \"Python_advanced\"     \"Python_beginner\"    \r\n\r\nstudent_courses <- student_courses %>%\r\n  mutate(studied_programming = case_when(str_detect(course, \"^R_\") ~ \"Studied R\",\r\n                                         str_detect(course, \"^Python_\") ~ \"Studied Python\",\r\n                                         TRUE ~ \"No\"))    \r\n\r\n# Remove newly created columns using dplyr -------------------------------------  \r\nstudent_courses <- student_courses %>%\r\n  select(-c(platform_dwell_length,\r\n            platform_start_year, \r\n            studied_programming))\r\n\r\n\r\n\r\nData frame outputs are slightly different in data.table:\r\nColumn transformations are always modified in place using the operator :=.\r\nMultiple columns can be concurrently modified, as long as the newly created columns do not depend on each other.\r\nUse subassignment to only extract the columns transformed inside j of DT[i, j, by].\r\n\r\n\r\n# Create column(s) using data.table --------------------------------------------  \r\nstudent_courses[,\r\n                `:=` (platform_dwell_length = platform_end_date - platform_start_date,\r\n                      platform_start_year = str_extract(platform_start_date, \"^.{4}(?!//-)\"))] \r\n\r\nstudent_courses[,\r\n                platform_start_year := as.numeric(platform_start_year)]  \r\n\r\n# Create column(s) with multiple conditions using data.table -------------------\r\nstudent_courses[,\r\n                studied_programming := fcase(\r\n                  str_detect(course, \"^R_\"), \"Studied R\",\r\n                  str_detect(course, \"^Python_\"), \"Studied Python\",\r\n                  default = \"No\")]  \r\n\r\n# Remove newly created columns using data.table --------------------------------\r\n# Column(s) can be removed by assignment as NULL variable(s)  \r\nstudent_courses[,\r\n                c(\"platform_dwell_length\",\r\n                  \"platform_start_year\",\r\n                  \"studied_programming\") := NULL]\r\n\r\n\r\n\r\n\r\n\r\n# Only columns transformed inside j are kept ----------------------------------- \r\n# Use subassignment only if you want to subset the transformed columns     \r\nplatform_records <- student_courses[,\r\n                                    .(id = toupper(id),\r\n                                      platform = tolower(platform))]  \r\n\r\nncol(platform_records)\r\n#> [1] 2  \r\n\r\n\r\n\r\nBenchmark column creation\r\n\r\nTable 4: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nmutate(student_courses, platform_dwell_length = platform_end_date - platform_start_date)\r\n4.6293\r\n7.76910\r\n38.18618\r\n9.09385\r\n13.0911\r\n287.4840\r\n100\r\nstudent_courses[, :=(“platform_dwell_length”, platform_end_date - platform_start_date)]\r\n3.4639\r\n4.95785\r\n22.75289\r\n6.51215\r\n8.9081\r\n282.2970\r\n100\r\ndplyr_case_when\r\n258.4914\r\n296.83920\r\n359.38240\r\n330.48085\r\n402.0158\r\n608.2437\r\n100\r\ndata.table_fcase\r\n211.6162\r\n240.15240\r\n280.84042\r\n262.18765\r\n312.7383\r\n443.5078\r\n100\r\n\r\nSimple group by operations\r\nUsing dplyr versus data.table\r\nEvaluating summary characteristics for data groupings is also where data.table significantly outperforms dplyr. A grouping is specified using the group_by() function in dplyr and inside the by placeholder of DT[i, j, by] in data.table.\r\n\r\n\r\n# Find total number of courses per student via dplyr ---------------------------    \r\nstudent_courses %>%\r\n  group_by(id) %>%\r\n  summarise(total_courses = n()) %>%\r\n  ungroup()\r\n\r\n# Code above is also equivalent to using count()   \r\nstudent_courses %>%\r\n  count(id)\r\n\r\n# Find total number of distinct courses per student via dplyr ------------------   \r\nstudent_courses %>%\r\n  group_by(id) %>%\r\n  summarise(total_distinct_courses = n_distinct(course)) %>%\r\n  ungroup() \r\n\r\n# Find the first course studied per student and platform via dplyr -------------\r\nstudent_courses %>%\r\n  group_by(id, platform) %>% # Group by two variables    \r\n  filter(row_number() == 1L) %>% # Return the first row from each group    \r\n  ungroup()   \r\n\r\n\r\n\r\n\r\n\r\n# Find total number of courses per student via data.table ----------------------    \r\nstudent_courses[,\r\n                .(total_courses = .N),\r\n                by = id]   \r\n\r\n# Find total number of distinct courses per student via data.table -------------   \r\nstudent_courses[,\r\n                .(total_distinct_courses = length(unique(course))),\r\n                by = id]   \r\n\r\n# uniqueN(x) is a data.table function equivalent to length((unique(x)) \r\nstudent_courses[,\r\n                .(total_distinct_courses = uniqueN(course)),\r\n                by = id]   \r\n\r\n# Find the first course studied per student and platform via data.table --------\r\nstudent_courses[,\r\n                .SD[1L],\r\n                by = .(id, platform)]\r\n\r\n\r\n\r\nBenchmark simple group by operations\r\nYou can clearly see that groupings are significantly faster in data.table than dplyr.\r\nNote: Not all data.table functions outperform their base R or dplyr equivalents. The data.table uniqueN(x) function is much slower than length(unique(x)).\r\n\r\nTable 5: Units: milliseconds\r\nexpr\r\nmin\r\nlq\r\nmean\r\nmedian\r\nuq\r\nmax\r\nneval\r\nstudent_courses %>% group_by(id) %>% summarise(total_courses = n())\r\n2253.7473\r\n2416.8814\r\n2848.64304\r\n2498.8589\r\n3270.4622\r\n4162.8841\r\n10\r\nstudent_courses[, .(total_courses = .N), by = id]\r\n46.7541\r\n50.0241\r\n60.37993\r\n53.6803\r\n70.2789\r\n83.9196\r\n10\r\nstudent_courses %>% group_by(id) %>% summarise(total_distinct_courses = n_distinct(course))\r\n4025.6523\r\n4237.2057\r\n4937.93044\r\n4647.9254\r\n5331.3878\r\n6326.4315\r\n10\r\nstudent_courses[, .(total_distinct_courses = length(unique(course))), by = id]\r\n847.1269\r\n879.6714\r\n1034.19212\r\n971.1114\r\n1218.1229\r\n1356.3699\r\n10\r\nstudent_courses[, .(total_distinct_courses = uniqueN(course)), by = id]\r\n6262.0236\r\n6343.8344\r\n7207.72692\r\n6594.7426\r\n7264.5609\r\n10044.3720\r\n10\r\nstudent_courses %>% group_by(id, platform) %>% filter(row_number() == 1L)\r\n7872.6713\r\n8103.4138\r\n9778.25568\r\n9260.2453\r\n10907.8334\r\n13687.8050\r\n10\r\nstudent_courses[, .SD[1L], by = .(id, platform)]\r\n69.1016\r\n71.2565\r\n78.52564\r\n74.1029\r\n78.2506\r\n120.5737\r\n10\r\n\r\nSummary\r\nMost data.table operations significantly outperform their dplyr equivalents in processing speed. I use data.table when grouping on large datasets (i.e. on datasets with greater than ~ 0.5 million rows) and use dplyr for day-to-day analyses of smaller datasets.\r\nOther resources\r\nA stack overflow discussion about the best use cases for data.table versus dplyr.\r\nA great side-by-side comparison of data.table versus dplyr functions from a blog post by Atrebas.\r\nA list of advanced data.table operations and tricks from a blog post by Andrew Brooks.\r\nAn explanation of how data.table modifies by reference from a blog post by Tyson Barrett.\r\nA benchmark of dplyr versus data.table functions from a blog post by Tyson Barrett\r\n\r\nI am extremely thankful for encountering these Twitter debates, as they helped draw more attention to data.table usage.↩︎\r\nFor accurate benchmarking, you need to separately run, save and print microbenchmark() outputs rather than directly knitting results.↩︎\r\n",
    "preview": "posts/2021-01-30-data-table-part-1/finalplot.png",
    "last_modified": "2021-04-05T21:09:12+10:00",
    "input_file": "data-table-part-1.utf8.md",
    "preview_width": 2205,
    "preview_height": 1418
  },
  {
    "path": "posts/2021-01-02-volcano-plots-with-ggplot2/",
    "title": "Volcano plots with ggplot2",
    "description": "Revising my grammar of graphics.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "data visualisation",
      "ggplot2",
      "tidyverse",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nImport a test dataset\r\nCreate a simple volcano plot\r\nAdd horizontal and vertical plot lines\r\nModify the x-axis and y-axis\r\nAdd colour, size and transparency\r\nLayer subplots\r\nLabel points of interest\r\nModify legend label positions\r\nModify plot labels and theme\r\nAnnotate text\r\nOther resources\r\n\r\nIntroduction\r\nIn 2018, whilst still an R newbie, I participated in the RLadies Melbourne community lightning talks and talked about how to visualise volcano plots in R. Volcano plots are probably an obscure concept outside of bioinformatics, but their construction nicely showcases the elegance of ggplot2.\r\nIn the last two years, a number of small and handy functions have been added to dplyr and ggplot2, which this post has been updated to reflect. 1\r\nLet’s get started then.\r\n\r\n\r\n# Load required packages -------------------------------------------------------  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,  \r\n               tidyverse, \r\n               janitor, # Cleaning column names  \r\n               scales, # Transform axis scales   \r\n               ggrepel) # Optimise plot label separation  \r\n\r\n\r\n\r\nImport a test dataset\r\nThis is a dataset with four columns:\r\nEntrez ID - stores the unique gene ID.\r\nGene symbol - stores the gene name associated with an unique Entrez ID.\r\nFold change - stores the change in gene expression level detected in diseased versus healthy tissue.\r\nAdjusted P-value - stores the P-value adjusted with a false discovery rate (FDR) correction for multiple testing.\r\nEvery row represents a unique gene expression fold change, which fulfills tidy data requirements for creating data visualisations.\r\nNote: The data used originates from Fu et al. Nat Cell Biol. 2015 and a copy of the original dataset can be found here.\r\n\r\n\r\n# Import and clean dataset ----------------------------------------------------- \r\ndiseased_vs_healthy <- read.delim(here(\"data\", \"luminal-pregnant-vs-lactate.txt\"),\r\n                                  header = TRUE,\r\n                                  sep = \"\\t\")  \r\n\r\ndiseased_vs_healthy <- janitor::clean_names(diseased_vs_healthy)  \r\n\r\ndiseased_vs_healthy <- diseased_vs_healthy %>%\r\n  mutate(fold_change = 2^log_fc) %>%\r\n  select(entrezid,\r\n         symbol,\r\n         fold_change,\r\n         adj_p_val)  \r\n\r\n\r\n\r\n\r\nentrezid\r\nsymbol\r\nfold_change\r\nadj_p_val\r\n14367\r\nFzd5\r\n2.0716416\r\n0.0542520\r\n244144\r\nUsp35\r\n0.4197075\r\n0.0598292\r\n100216534\r\nSnord96a\r\n1.2373480\r\n0.9999999\r\n66151\r\nPrr13\r\n1.4296409\r\n0.9999999\r\n229445\r\nCtso\r\n1.0506197\r\n0.9999999\r\n\r\nCreate a simple volcano plot\r\nA basic version of a volcano plot depicts:\r\nAlong its x-axis: log2(fold_change)\r\nAlong its y-axis: -log10(adj_p_val)\r\nNote: The y-axis depicts -log10(adj_p_val), which allows the points on the plot to project upwards as the fold change greatly increases or decreases. This is more intuitive to visualise, the data points at the edges of the ‘volcano spray’ are the most interesting ones.\r\nThe versatility of ggplot2 also means that you don’t need to store data transformations as separate variables for plotting. You can apply transformations directly inside ggplot(data, aes(x, y)) or alternatively by using scale_x_continuous(trans = \"...\") or coord_trans(x, y).\r\n\r\n\r\n# Create a simple volcano plot -------------------------------------------------\r\nvol_plot <- diseased_vs_healthy %>%\r\n  ggplot(aes(x = log2(fold_change),\r\n             y = -log10(adj_p_val))) + \r\n  geom_point() \r\n\r\nvol_plot # Visualise simple volcano plot  \r\n\r\n\r\n\r\n\r\nNote: For single layer plots, use %>% pipes with ggplot2 functions for convenience and readability.\r\nAdd horizontal and vertical plot lines\r\nThe functions geom_hline() and geom_vline() can be used to add extra horizontal and vertical lines on your plot respectively. In this example, I am interested in constructing boundaries for genes which have adj_p_val <= 0.05 and fold_change <= 0.5 or fold_change >= 2.\r\n\r\n\r\n# Plot extra quadrants ---------------------------------------------------------\r\nvol_plot + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\")   \r\n\r\n\r\n\r\n\r\nModify the x-axis and y-axis\r\nVolcano plots should have a symmetrical x-axis. One way you can do this is by manually setting the limits of the x-axis using xlim(min, max).\r\n\r\n\r\n# Identify xlim() values -------------------------------------------------------\r\ndiseased_vs_healthy %>%\r\n  pull(fold_change) %>%\r\n  min() %>%\r\n  log2() %>%\r\n  floor() \r\n#> [1] -10   \r\n\r\ndiseased_vs_healthy %>%\r\n  pull(fold_change) %>%\r\n  max() %>%\r\n  log2() %>%\r\n  ceiling()\r\n#> [1] 8  \r\n\r\nmax(abs(-10), abs(8))\r\n#> [1] 10  \r\n\r\n# Change xlim() ----------------------------------------------------------------  \r\n# Manually specify x-axis limits   \r\nvol_plot + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") + \r\n  xlim(-10, 10) \r\n\r\n\r\n\r\n\r\nYou can also change the limits of the x-axis inside scale_x_continuous. This method also gives you the flexibility to fine-tune the spacing and labelling of axis tick marks.\r\n\r\n\r\n# Modify scale_x_continuous() --------------------------------------------------\r\nvol_plot + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)), # Modify x-axis tick intervals    \r\n                     limits = c(-10, 10)) \r\n\r\n\r\n\r\n\r\nNote: The value specified inside the argument scale_continuous_x(limits = ...) supersedes the range of values specified inside the argument scale_continuous_x(breaks = ...).\r\nAdd colour, size and transparency\r\nTo visualise different groups of genes using different colours, point sizes, shapes or transparencies, you need to categorise genes into different groups and store these categories as a new parameter i.e. new column of data.\r\nI am interested in labelling genes into the following groups:\r\nGenes with a fold change >= 2 and adjusted p-value <= 0.05 labelled as ‘up’.\r\nGenes with a fold change <= 0.5 and adjusted p-value <= 0.05 labelled as ‘down’.\r\nAll other genes labelled as ‘ns’ i.e. non-significant.\r\n\r\n\r\n# Create new categorical column ------------------------------------------------ \r\ndiseased_vs_healthy <- diseased_vs_healthy %>%\r\n  mutate(gene_type = case_when(fold_change >= 2 & adj_p_val <= 0.05 ~ \"up\",\r\n                               fold_change <= 0.5 & adj_p_val <= 0.05 ~ \"down\",\r\n                               TRUE ~ \"ns\"))   \r\n\r\n# Obtain gene_type counts ------------------------------------------------------           \r\ndiseased_vs_healthy %>%\r\n  count(gene_type)\r\n\r\n\r\n\r\n\r\ngene_type\r\nn\r\ndown\r\n1245\r\nns\r\n13578\r\nup\r\n981\r\n\r\nIn ggplot2, you also have the option to visualise different groups by point colour, size, shape and transparency by modifying parameter like scale_color_manual() etc. A tidy way of doing this is to separately store your manual specifications as vectors.\r\n\r\n\r\n# Check gene_type categories ---------------------------------------------------    \r\ndiseased_vs_healthy %>%\r\n  distinct(gene_type) %>%\r\n  pull()  \r\n#> [1] \"down\" \"up\"   \"ns\"    \r\n\r\n\r\n\r\n\r\n\r\n# Add colour, size and alpha (transparency) to volcano plot -------------------- \r\ncols <- c(\"up\" = \"#ffad73\", \"down\" = \"#26b3ff\", \"ns\" = \"grey\") \r\nsizes <- c(\"up\" = 2, \"down\" = 2, \"ns\" = 1) \r\nalphas <- c(\"up\" = 1, \"down\" = 1, \"ns\" = 0.5)\r\n\r\ndiseased_vs_healthy %>%\r\n  ggplot(aes(x = log2(fold_change),\r\n             y = -log10(adj_p_val),\r\n             fill = gene_type,    \r\n             size = gene_type,\r\n             alpha = gene_type)) + \r\n  geom_point(shape = 21, # Specify shape and colour as fixed local parameters    \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  scale_fill_manual(values = cols) + # Modify point colour\r\n  scale_size_manual(values = sizes) + # Modify point size\r\n  scale_alpha_manual(values = alphas) + # Modify point transparency\r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),       \r\n                     limits = c(-10, 10))  \r\n\r\n\r\n\r\n\r\nLayer subplots\r\nYou can also overlay subplots on top of your main plot. This is useful when you want to highlight a subset of your data using different colours, shapes and etc. When overlaying plots, you should not use %>% pipes but use global ggplot(data = “…”) and local geom_point(data = ...) arguments instead.\r\n\r\n\r\n# Add subplot layer to the main volcano plot -----------------------------------  \r\nils <- str_subset(diseased_vs_healthy$symbol, \"^[I|i]l[0-9]+$\")  \r\n\r\nil_genes <- diseased_vs_healthy %>%\r\n  filter(symbol %in% ils) \r\n\r\nggplot(data = diseased_vs_healthy, # Original data  \r\n       aes(x = log2(fold_change), y = -log10(adj_p_val))) + \r\n  geom_point(colour = \"grey\", alpha = 0.5) +\r\n  geom_point(data = il_genes, # New layer containing data subset il_genes       \r\n             size = 2,\r\n             shape = 21,\r\n             fill = \"firebrick\",\r\n             colour = \"black\")     \r\n\r\n\r\n\r\n\r\nNote: Unless local aesthetics are specified, secondary geom_point() functions will inherit global ggplot aesthetics.\r\nLabel points of interest\r\nYou can also label a subset of data using geom_text(), geom_label(), geom_text_repel() or geom_label_repel and by specifying which column to display as text using the local argument geom_text(aes(label = ...)).\r\nNote: adjusting the parameters for optimal text separation using geom_text_repel can be a bit fiddly. I generally start by modifying force and then deciding which region of the plot I want to nudge my text or labels towards. You can read this vignette for more tips on adjusting geom_text_repel parameters.\r\n\r\n\r\n# Layer more subplots ----------------------------------------------------------\r\nsig_il_genes <- diseased_vs_healthy %>%\r\n  filter(symbol %in% c(\"Il15\", \"Il34\", \"Il24\"))\r\n\r\nup_il_genes <- diseased_vs_healthy %>%\r\n  filter(symbol == \"Il24\")\r\n\r\ndown_il_genes <- diseased_vs_healthy %>%\r\n  filter(symbol %in% c(\"Il15\", \"Il34\"))\r\n\r\nggplot(data = diseased_vs_healthy,\r\n       aes(x = log2(fold_change),\r\n           y = -log10(adj_p_val))) + \r\n  geom_point(aes(colour = gene_type), \r\n             alpha = 0.2, \r\n             shape = 16,\r\n             size = 1) + \r\n  geom_point(data = up_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"firebrick\", \r\n             colour = \"black\") + \r\n  geom_point(data = down_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"steelblue\", \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  geom_label_repel(data = sig_il_genes, # Add labels last to appear as the top layer  \r\n                   aes(label = symbol),\r\n                   force = 2,\r\n                   nudge_y = 1) +\r\n  scale_colour_manual(values = cols) + \r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),     \r\n                     limits = c(-10, 10))  \r\n\r\n\r\n\r\n\r\nModify legend label positions\r\nIf you need to change the order of categorical figure legend values, you will need to factor() and re-level your categorical variable. This can be done using the forcats package, which allows you to easily modify factor levels.\r\n\r\n\r\n# Modify legend labels by re-ordering gene_type levels -------------------------  \r\ndiseased_vs_healthy <- diseased_vs_healthy %>%\r\n  mutate(gene_type = fct_relevel(gene_type, \"up\", \"down\")) \r\n\r\n# Recreate volcano plot --------------------------------------------------------  \r\nggplot(data = diseased_vs_healthy,\r\n       aes(x = log2(fold_change),\r\n           y = -log10(adj_p_val))) + \r\n  geom_point(aes(colour = gene_type), \r\n             alpha = 0.2, \r\n             shape = 16,\r\n             size = 1) + \r\n  geom_point(data = up_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"firebrick\", \r\n             colour = \"black\") + \r\n  geom_point(data = down_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"steelblue\", \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  geom_label_repel(data = sig_il_genes,     \r\n                   aes(label = symbol),\r\n                   force = 2,\r\n                   nudge_y = 1) +\r\n  scale_colour_manual(values = cols) + \r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),     \r\n                     limits = c(-10, 10))   \r\n\r\n\r\n\r\n\r\nModify plot labels and theme\r\nThe last finishing touches include modifying plot labels and the plot theme.\r\nThe function labs() is a handy way of organising all plot labels inside a single function. You can assign labels as NULL to prevent them from being displayed.\r\nA plot can be further improved by changing its theme() and/or by modifying individual theme() parameters.\r\n\r\n\r\n# Add plot labels and modify plot theme ----------------------------------------\r\nfinal_plot <- ggplot(data = diseased_vs_healthy,\r\n       aes(x = log2(fold_change),\r\n           y = -log10(adj_p_val))) + \r\n  geom_point(aes(colour = gene_type), \r\n             alpha = 0.2, \r\n             shape = 16,\r\n             size = 1) + \r\n  geom_point(data = up_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"firebrick\", \r\n             colour = \"black\") + \r\n  geom_point(data = down_il_genes,\r\n             shape = 21,\r\n             size = 2, \r\n             fill = \"steelblue\", \r\n             colour = \"black\") + \r\n  geom_hline(yintercept = -log10(0.05),\r\n             linetype = \"dashed\") + \r\n  geom_vline(xintercept = c(log2(0.5), log2(2)),\r\n             linetype = \"dashed\") +\r\n  geom_label_repel(data = sig_il_genes,   \r\n                   aes(label = symbol),\r\n                   force = 2,\r\n                   nudge_y = 1) +\r\n  scale_colour_manual(values = cols) + \r\n  scale_x_continuous(breaks = c(seq(-10, 10, 2)),     \r\n                     limits = c(-10, 10)) +\r\n  labs(title = \"Gene expression changes in diseased versus healthy samples\",\r\n       x = \"log2(fold change)\",\r\n       y = \"-log10(adjusted P-value)\",\r\n       colour = \"Expression \\nchange\") +\r\n  theme_bw() + # Select theme with a white background  \r\n  theme(panel.border = element_rect(colour = \"black\", fill = NA, size= 0.5),    \r\n        panel.grid.minor = element_blank(),\r\n        panel.grid.major = element_blank()) \r\n\r\nfinal_plot \r\n\r\n\r\n\r\n\r\nNote: You can specify panel.grid... = element_line(linetype = \"dotted\") inside theme() to create dotted gridlines along the x and/or y axis. Major gridline positions are inherited from the values of axis breaks.\r\nAnnotate text\r\nYou can add more descriptions to a plot by using the function annotate() to display text.\r\n\r\n\r\n# Annotate text inside plot ----------------------------------------------------\r\nfinal_plot + \r\n  annotate(\"text\", x = 7, y = 10,\r\n           label = \"3 interleukins of interest\", color = \"firebrick\")\r\n\r\n\r\n\r\n\r\nOther resources\r\nThe excellent and interactive code-along RStudio Cloud ggplot2 tutorials\r\nRStudio ggplot cheatsheet\r\nSTHDA tutorial on ggplot2 axis transformations\r\n\r\nThe original coding logic should still be attributed to Chuanxin Liu, my former PhD student. I also recommend the excellent RStudio Cloud ggplot2 tutorials, which have taught me a few new tricks.↩︎\r\n",
    "preview": "posts/2021-01-02-volcano-plots-with-ggplot2/finalplot.png",
    "last_modified": "2021-04-05T19:43:37+10:00",
    "input_file": {},
    "preview_width": 2187,
    "preview_height": 1350
  },
  {
    "path": "posts/2020-12-31-cleaning-free-text-and-wrangling-strings/",
    "title": "Cleaning free text and wrangling strings",
    "description": "These are some common data cleaning things.",
    "author": [
      {
        "name": "Erika Duan",
        "url": {}
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "data cleaning",
      "regex",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCreate a test dataset\r\nIntroduction to regular expressions\r\nMatch characters\r\nCharacter anchors\r\nCharacter classes and groupings\r\nGreedy versus lazy matches\r\nLook arounds\r\n\r\nImprove comment field readability\r\nManually extract topics of interest\r\nExtract a machine learning friendly dataset\r\nDifferences between base R and stringr functions\r\nOther resources\r\n\r\nIntroduction\r\nComment fields sit somewhere in between tidy tabular data entries and large text files (i.e. documents) in terms of wrangling effort. They require human naunce to decode and the quality and completeness of comments vary between individual entries.\r\nThis makes it hard to gauge whether cleaning comment fields is a worthwhile endeavour (especially when you have multiple other data sources that need examining). Luckily, some knowledge of string manipulations and regular expressions can help simplify this process.\r\nLet’s get started.\r\n\r\n\r\n# Load required packages -------------------------------------------------------  \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(here,  \r\n               tidyverse,  \r\n               microbenchmark) \r\n\r\n\r\n\r\nCreate a test dataset\r\nLet’s imagine that my local chocolate company, Haighs Chocolates, wants to understand what food critics versus Haighs fans think about their newest product. They send out a bag of free samples with a link to an online survey that asks individuals to rate their chocolates (on a scale of 1 to 10) and provide additional comments.\r\nNote: The code used to create this survey can be accessed from my github repository here.\r\n\r\n\r\n\r\n\r\n\r\n# Examine the first 6 rows of data ---------------------------------------------  \r\nsurvey %>%\r\n  head(6)  \r\n\r\n\r\n# A tibble: 6 x 3\r\n  respondee rating comment_field                                      \r\n  <chr>     <chr>  <chr>                                              \r\n1 expert_1  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n2 expert_2  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n3 expert_3  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n4 expert_4  10     \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n5 expert_5  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n6 fan_1     9      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> Delicious ~\r\n\r\nOh dear, it looks like we will first need to use regular expressions to remove all the html tags embedded within survey$comment_field.\r\nIntroduction to regular expressions\r\nRegular expressions, or regex, can be thought of as a separate syntax for handling patterns in strings. In R, regular expressions can be directly enclosed inside quotes like character strings or explicitly referenced inside regex(). For convenience, I prefer the former approach but the latter approach can help increase code readability.\r\n\r\n\r\n# Call regular expressions in R ------------------------------------------------\r\nmany_apples <- c(\"Apple\", \"apple\", \"APPLE\", \"apples\")\r\n\r\nstr_extract(many_apples, \"apples?\")  \r\n#> [1] NA       \"apple\"  NA       \"apples\"\r\n\r\n# Call regular expressions in R using regex() ----------------------------------\r\n# regex() provides additional arguments\r\n\r\nstr_extract(many_apples, regex(\"apples?\", ignore_case = T))  \r\n#> [1] \"Apple\"  \"apple\"  \"APPLE\"  \"apples\"  \r\n\r\n\r\n\r\nMatch characters\r\nSome sequences of characters have specific meanings. For example, s refers to the letter \"s\" but \\s refers to any type of white space. To call whitespace in R, a second backslash \\ is required to escape special character behaviour i.e. \\\\s.\r\n\r\n\r\n# Examples of special character sequences --------------------------------------  \r\nwords_and_spaces <- c(\" a cat\",\r\n                      \"acat\",\r\n                      \"a   cat\",\r\n                      \"a\\ncat\",\r\n                      \"a\\\\ncat\")\r\n\r\n# \"a\\\\s+cat\" calls variations of \"a...cat\" separated by one or more whitespaces \r\n# Note that the string \"a\\ncat\" also counts because \\n refers to a new line\r\n\r\nstr_extract(words_and_spaces, \"a\\\\s+cat\")  \r\n#> [1] \"a cat\"   NA        \"a   cat\" \"a\\ncat\"  NA      \r\n\r\n# \"\\\\S+\" refers to everything that is not white space (starting from left to right)  \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"\r\n\r\n\r\n\r\nNote: The special characters \\s versus \\S allow the extraction of opposite pattern types. In another example, lowercase \\w refers to any word character whilst uppercase \\W and lowercase [^\\w] both refer to anything that is not a word character.\r\nCharacter anchors\r\nI feel that the goal of writing good regex is to be as specific as possible. This is why character anchors are useful (i.e. using ^ and $ to denote the start and end of your string respectively).\r\nIf we re-visit the example above, we can see that the presence or absence of character anchors produces very different outputs.\r\n\r\n\r\n# Investigate impact of character anchors --------------------------------------    \r\nwords_and_spaces <- c(\" a cat\",\r\n                      \"acat\",\r\n                      \"a   cat\",\r\n                      \"a\\ncat\",\r\n                      \"a\\\\ncat\")\r\n\r\n# \"\\\\S+\" refers to all non-white space read from left to right  \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+\")  \r\n#> [1] \"a\"       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"  \r\n\r\nstr_extract(words_and_spaces, \"^\\\\S+\")  \r\n#> [1] NA       \"acat\"    \"a\"       \"a\"       \"a\\\\ncat\"   \r\n\r\nstr_extract(words_and_spaces, \"\\\\S+$\") \r\n#> [1] \"cat\"     \"acat\"    \"cat\"     \"cat\"     \"a\\\\ncat\"       \r\n\r\n\r\n\r\nCharacter classes and groupings\r\nCharacter classes and groupings are handy for extracting specific letter and/or digit combinations. Some special characters found inside character classes and groupings are:\r\nThe operation or is represented by | i.e [a|c]\r\nThe operation range is represented by - i.e. [a-z]\r\nThe operation excludes is represented by ^ i.e. [^a-c]\r\nNote: Representation of a single character is denoted by [] and representation of a grouping i.e. combination of characters is denoted by ().\r\n\r\n\r\n# Extract patterns using character classes [] ----------------------------------    \r\nstrange_fruits <- c(\"apple1\",\r\n                    \"bapple2\",\r\n                    \"capple3\",\r\n                    \"dapple4\",\r\n                    \"epple5\",\r\n                    \"aggle0\")\r\n\r\nstr_extract(strange_fruits, \"[a-d]\")\r\n#> [1] \"a\" \"b\" \"c\" \"d\" NA  \"a\"  \r\n\r\nstr_extract(strange_fruits, \"[a-d][^p]\")\r\n#> [1] NA   \"ba\" \"ca\" \"da\" NA   \"ag\"   \r\n\r\n# [a-d][^p] refers to a character between a to d followed by a character that is not p  \r\n\r\nstr_extract(strange_fruits, \"[0|4-9]\")\r\n#> [1] NA  NA  NA  \"4\" \"5\" \"0\"   \r\n\r\n# [0|4-9] refers to a number that is zero or a number between 4 to 9    \r\n\r\n\r\n\r\n\r\n\r\n# Extract patterns using groupings () ------------------------------------------     \r\nstrange_fruits <- c(\"apple1\",\r\n                    \"bapple2\",\r\n                    \"capple3\",\r\n                    \"dapple4\",\r\n                    \"epple5\",\r\n                    \"aggle1\")  \r\n\r\nstr_extract(strange_fruits, \"a(pp|gg)le\")\r\n#> [1] \"apple\" \"apple\" \"apple\" \"apple\" NA      \"aggle\"    \r\n\r\n# Groups can be referenced by their order of appearance i.e. \\\\1 = first group  \r\n\r\nstr_extract(strange_fruits, \"(a)(p|g)\\\\2\")\r\n#> [1] \"app\" \"app\" \"app\" \"app\" NA    \"agg\"   \r\n\r\n# (a) is group 1 and can be called using \\\\1    \r\n# (p|g) is group 2 and can be called using \\\\2     \r\n\r\n\r\n\r\nGreedy versus lazy matches\r\nIn R, regular expression parsing is non-greedy by default. This means that we need to add quantifiers * and + to greedily extract zero or more and one or more characters respectively.\r\nUsing a non-greedy match allows you to extract just the first characters before a white space or punctuation mark. This is useful for trimming strings or extracting file or object names.\r\n\r\n\r\n\r\n\r\n\r\n# Examples of greedy matches ---------------------------------------------------   \r\nmessy_dates <- c(\"Thursday 24th May\",\r\n                 \"Thursday  24th May  \",\r\n                 \" May\",\r\n                 \"May    \")\r\n\r\nstr_extract(messy_dates, \"^\\\\w\")      \r\n#> [1] \"T\" \"T\" NA  \"M\"   \r\n\r\n# Greedily extract the first word in the string    \r\n\r\nstr_extract(messy_dates, \"^\\\\w+\")   \r\n#> [1] \"Thursday\" \"Thursday\" NA      \"May\"   \r\n\r\n# The quantifier + and {1,} are equivalent   \r\n\r\nstr_extract(messy_dates, \"^\\\\w{1,}\")  \r\n#> [1] \"Thursday\" \"Thursday\" NA      \"May\"    \r\n\r\nstr_extract(messy_dates, \"^(\\\\S+)\")  \r\n#> [1] \"Thursday\" \"Thursday\" NA      \"May\"    \r\n\r\n# Examples of non-greedy matches ----------------------------------------------- \r\nstr_replace_all(messy_dates, \"\\\\s\" , \"-\") # Replaces each individual whitespace\r\n#> [1] \"Thursday-24th-May\"    \"Thursday--24th-May--\" \"-May\"            \"May----\"       \r\n\r\nstr_replace_all(messy_dates, \"\\\\s{1,2}\" , \"-\") \r\n#> [1] \"Thursday-24th-May\"  \"Thursday-24th-May-\" \"-May\"                  \"May--\"         \r\n\r\n# Use look-arounds to replace the whitespace(s) after the first word     \r\n\r\nstr_replace_all(messy_dates, \"(?<=^\\\\w{1,2})\\\\s{1,2}\" , \"-\") \r\n#> [1] \"Thursday-24th May\"   \"Thursday-24th May  \" \" May\"               \"May-  \"     \r\n\r\n\r\n\r\nNote: For a deeper explanation of the regex syntax for the last example, read this stack overflow post.\r\nLook arounds\r\nLook around operations are useful when you are unsure of the pattern itself, but you know exactly what its preceding or following pattern is. I’ve found that the clearest explanation of look around operations comes from the RStudio cheetsheet on string_r, as depicted below.\r\n\r\n\r\n\r\nFigure 1: Taken from the RStudio stringr cheatsheet\r\n\r\n\r\n\r\n\r\n\r\n# Examples of different types of look arounds ----------------------------------  \r\nrecipes <- c(\"croissant recipes\",\r\n             \"apple pie recipe\",\r\n             \"chocolate cake  recipe\", # extra space\r\n             \"cookie receipe\",  # deliberate typo\r\n             \"secret KFC-recipe\", \r\n             \"very secret  McDonalds soft-serve recipe\") # extra space  \r\n\r\n# use positive look-ahead (?=...) to extract the preceding word\r\n\r\nstr_extract(recipes, \"\\\\S+(?=\\\\s*recipes?)\")   \r\n#> [1] \"croissant\"  \"pie\"        \"cake\"       NA           \"KFC-\"       \"soft-serve\"   \r\n\r\n# use positive look-behind (?<=) on \"secret\" to identify the secret recipes  \r\n\r\nstr_extract(recipes, \"(?<=secret\\\\s{1,10})\\\\S+.+\")   \r\n#> [1] NA                            NA                            NA                           \r\n#> [4] NA                            \"KFC-recipe\"                  \"McDonalds soft-serve recipe\"   \r\n\r\n\r\n\r\nNote: Positive look-behinds require defined boundary specifications i.e. the operation + needs to be converted into {1,1000}.\r\nImprove comment field readability\r\nWith regex revised, let us return to our Haighs chocolate survey. The first thing we can see is that html tags have been retained inside the comment field and that this field is very long (i.e. difficult to read).\r\nWe can improve the readability of the survey by:\r\nRemoving all html tags using regex.\r\nSeparating phrases into columns using separate().\r\n\r\n\r\n# Examine survey data ----------------------------------------------------------\r\nsurvey %>%\r\n  head(5)   \r\n\r\n\r\n# A tibble: 5 x 3\r\n  respondee rating comment_field                                      \r\n  <chr>     <chr>  <chr>                                              \r\n1 expert_1  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n2 expert_2  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n3 expert_3  8      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n4 expert_4  10     \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n5 expert_5  7      \"<textarea name=\\\"comment\\\" form=\\\"1\\\"> &lt;Grade ~\r\n\r\n# Remove html tags -------------------------------------------------------------\r\nremove_html_tags <- regex(\"\r\n                          <  # starts with <\r\n                          [^>]+  # contains one or more of all characters excepting > \r\n                          >  # ends with >\r\n                          \", comments = T)\r\n\r\nremove_more_html <- regex(\"\r\n                          \\\\& # starts with &\r\n                          \\\\w+ # contains one or more word characters\r\n                          \\\\; # ends with ;\r\n                          \", comments = T) \r\n\r\nsurvey <- survey %>%\r\n  mutate(comment_field = str_replace_all(comment_field, remove_html_tags, \"\"),\r\n         comment_field = str_replace_all(comment_field, remove_more_html, \"\"))\r\n\r\n# Examine cleaned comment_field ------------------------------------------------  \r\nsurvey %>%\r\n  select(comment_field) %>%\r\n  head(5) \r\n\r\n\r\n# A tibble: 5 x 1\r\n  comment_field                                                       \r\n  <chr>                                                               \r\n1 \" Grade A beans. Easily melts. Smooth chocolate shell, with a crunc~\r\n2 \" Grade A beans with subtle caramel hints. Melts well. Smooth exter~\r\n3 \" Grade a beans.  Caramel and vanilla undertones complement the bit~\r\n4 \" Grade A cocoa beans. Melts easily. Smooth dark chocolate contrast~\r\n5 \" Grade A beans, likely of Ecuador origin. Smooth dark chocolate co~\r\n\r\nWe can then split the single long comment field into multiple smaller columns. 1\r\n\r\n\r\n# Separate comment_field into individual columns -------------------------------    \r\n# Separate on punctuation or conjunctions  \r\nnmax <- max(str_count(survey$comment_field, \"[[:punct:]]|and|with|against\")) + 1\r\n\r\nsurvey <- survey %>%   \r\n  separate(comment_field,\r\n           into = paste0(\"Field\", seq_len(nmax)),\r\n           sep = \"[[:punct:]]|and|with|against\",   \r\n           remove = F,\r\n           extra = \"warn\",\r\n           fill = \"right\") \r\n\r\n# Examine comment_fields -------------------------------------------------------  \r\nsurvey %>%\r\n  select(starts_with(\"Field\")) %>%\r\n  head(5) \r\n\r\n\r\n\r\nManually extract topics of interest\r\nAfter separating the comment field into smaller fields, we see references to:\r\ncocoa bean grade\r\npresence of caramel or vanilla flavour\r\nchocolate smoothness\r\nhow well the chocolate melts\r\nsugar content/ sweetness level\r\nmalt filling\r\nchocolate coating\r\nInformation about cocoa bean grade is highly structured. This means that extracting the letter following the word “Grade” is sufficient. A similar logic can be applied to extract whether caramel or vanilla flavour or chocolate smoothness was mentioned.\r\n\r\n\r\n# Extract information about cocoa bean grade, flavour and smoothness -----------\r\ntidy_survey <- survey %>%\r\n  select(respondee,\r\n         comment_field) %>% \r\n  mutate(cocoa_grade = str_extract(comment_field, \"(?<=[G|g]rade\\\\s{0,10})[A-C|a-c]\"),\r\n         is_caramel = case_when(str_detect(comment_field, \"[C|c]aramel\") ~ \"yes\",\r\n                                TRUE ~ NA_character_), \r\n         is_vanilla = case_when(str_detect(comment_field, \"[V|v]anilla\") ~ \"yes\",\r\n                                TRUE ~ NA_character_),\r\n         is_smooth = case_when(str_detect(comment_field, \"[S|s]mooth\") ~ \"yes\",\r\n                               TRUE ~ NA_character_)) \r\n\r\n# We cannot assign TRUE ~ NA inside case_when as NA is of logical type  \r\n\r\n\r\n\r\nFor more descriptive fields such as whether the chocolate melts, I find it easier to first extract a matrix of fields.\r\n\r\n\r\n# Extract information about chocolate texture ----------------------------------\r\nmelt_matrix <- survey %>%\r\n  select(respondee,\r\n         starts_with(\"Field\")) %>%  \r\n  mutate_at(vars(starts_with(\"Field\")),\r\n            ~replace(.x, !(str_detect(.x, \".*\\\\b[M|m]elt.*\\\\b.*\")), NA)) \r\n\r\n# Convert fields which do not contain \"melt\" into NA and unite fields     \r\n\r\nmelt_cols <- str_which(colnames(melt_matrix), \"^Field.+\")\r\n\r\nmelt_status <- melt_matrix %>%\r\n  unite(\"is_melty\", \r\n        all_of(melt_cols),  \r\n        sep = \"\",\r\n        remove = T,\r\n        na.rm = T) # Make sure to remove NAs    \r\n\r\n# Convert responses into factors and re-code factor levels ---------------------  \r\nmelt_status$is_melty <- factor(melt_status$is_melty)\r\n\r\nlevels(melt_status$is_melty) \r\n#> [1] \"\"        \" Easily melts\"        \" Melts easily\"   \r\n#> [4] \" melts in your mouth\"         \" Melts well\"         \r\n\r\nmelt_status <- melt_status %>%\r\n  mutate(is_melty = fct_collapse(is_melty,\r\n                                 \"yes\" = c(\" Easily melts\",\r\n                                           \" Melts well\",\r\n                                           \" Melts easily\",\r\n                                           \" melts in your mouth\"),\r\n                                 \"NA\" = \"\"))\r\n\r\n# Left join tidy_survey to melt_status -----------------------------------------  \r\ntidy_survey <- tidy_survey %>%\r\n  left_join(melt_status,\r\n            by = \"respondee\")\r\n\r\n\r\n\r\nThis process is repeated for chocolate sweetness. 2\r\n\r\n\r\n# Extract information about chocolate sweetness --------------------------------  \r\nsweetness_matrix <- survey %>%\r\n  select(respondee,\r\n         starts_with(\"Field\")) %>% \r\n  mutate_at(vars(starts_with(\"Field\")),\r\n            ~replace(.x, !(str_detect(.x, \".*\\\\b[S|s](weet)|(ugar).*\\\\b.*\")), NA)) \r\n\r\n# Convert fields which do not contain \"sweet\" or \"sugar\" into NA and unite fields     \r\n\r\nsweetness_cols <- str_which(colnames(sweetness_matrix), \"^Field.+\")\r\n\r\nsweetness_status <- sweetness_matrix %>%\r\n  unite(\"is_sweet\", \r\n        all_of(sweetness_cols), \r\n        sep = \"\",\r\n        remove = T,\r\n        na.rm = T) # Make sure to remove NAs  \r\n\r\n# Convert responses into factors and re-code factor levels --------------------- \r\nsweetness_status$is_sweet <- factor(sweetness_status$is_sweet)\r\n\r\nlevels(sweetness_status$is_sweet)\r\n#> [1] \"\"         \" low sugar content \"          \" not so sweet  I enjoyed this\"        \r\n#> [4] \"filled core may be too sweet for some\"\r\n\r\nsweetness_status <- sweetness_status %>%\r\n  mutate(is_sweet = fct_collapse(is_sweet,\r\n                                 \"yes\" = c(\"filled core may be too sweet for some\"),\r\n                                 \"no\" = c(\" low sugar content \",\r\n                                          \" not so sweet  I enjoyed this\"),\r\n                                 \"NA\" = \"\"))\r\n\r\n# Left join tidy_survey to melt_status -----------------------------------------  \r\ntidy_survey <- tidy_survey %>%\r\n  left_join(sweetness_status,\r\n            by = \"respondee\")\r\n\r\n\r\n\r\nNote: This method of converting topics into tabular variables works well when we are not dealing with too many factors (i.e. when recoding factors is not too cumbersome).\r\nExtract a machine learning friendly dataset\r\nA reason why we might be interested in converting unstructured comment fields into structured variables is to generate data features for machine learning. For instance, we might be interested in whether there is a relationship between survey topics, whether the comment comes from a critic or chocolate fan, and the chocolate rating.\r\n\r\n\r\n# Create final tidy_survey -----------------------------------------------------\r\nsurvey_rating <- survey %>%\r\n  select(respondee,\r\n         rating)  \r\n\r\ntidy_survey <- tidy_survey %>%\r\n  select(-comment_field) %>%\r\n  left_join(survey_rating,\r\n            by = \"respondee\") %>%\r\n  mutate(respondee = str_extract(respondee, \".+(?=\\\\_[0-9]+)\"))\r\n\r\nset.seed(123) # Sample reproducibly  \r\ntidy_survey %>%\r\n  sample_n(5)   \r\n\r\n\r\n\r\nDifferences between base R and stringr functions\r\nIn R, string manipulation can be performed using either base R functions or functions from the stringr library. A key difference between base R and stringr functions is the order that the string and pattern are specified. The pattern, not the string, is specified first inside base R functions, which is not a pipe friendly argument order.\r\n\r\n\r\n# Examples using grep() --------------------------------------------------------  \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngrep(\".*\\\\bchocolate\\\\b.*\", desserts, value = F) # Default is value = FALSE\r\n#> [1] 1 2 3 4 5 6  \r\n\r\n# grep(value = FALSE) only extracts the position of matching elements in the vector  \r\n\r\nstr_which(desserts, \".*\\\\bchocolate\\\\b.*\")  \r\n#> [1] 1 2 3 4 5 6  \r\n\r\ngrep(\".*\\\\bchocolate\\\\b.*\", desserts, value = T) \r\n#> [1] \"chocolate\"    \"chocolate cake\"    \"chocolate tart\"    \"chocolate icecream\"\r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"   \r\n\r\n# grep(value = TRUE) extracts the matching elements in the vector  \r\n\r\nstr_subset(desserts, \".*\\\\bchocolate\\\\b.*\") \r\n#> [1] \"chocolate\"    \"chocolate cake\"    \"chocolate tart\"    \"chocolate icecream\"  \r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"  \r\n\r\n# The function str_subset() is a wrapper around x[str_detect(x, pattern)]   \r\n\r\n\r\n\r\n\r\n\r\n# Examples using grepl() -------------------------------------------------------  \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngrepl(\".*\\\\bchocolate\\\\b.*\", desserts) \r\n#> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  \r\n\r\nstr_detect(desserts, \".*\\\\bchocolate\\\\b.*\")  \r\n#> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE \r\n\r\ndesserts[str_detect(desserts, \".*\\\\bchocolate\\\\b.*\")]\r\n#> [1] \"chocolate\"    \"chocolate cake\"   \"chocolate tart\"    \"chocolate icecream\"\r\n#> [5] \"chocolate cookies\"    \"dark chocolate fudge\"  \r\n\r\n\r\n\r\n\r\n\r\n# Examples using gsub() --------------------------------------------------------   \r\ndesserts <- c(\"chocolate\",\r\n              \"chocolate cake\",\r\n              \"chocolate tart\",\r\n              \"chocolate icecream\",\r\n              \"chocolate cookies\",\r\n              \"dark chocolate fudge\", \r\n              \"fruit\",\r\n              \"fruit tart\",\r\n              \"fruit sorbet\")\r\n\r\ngsub(\"(dark )?chocolate\", \"vanilla\", desserts) \r\n#> [1] \"vanilla\"          \"vanilla cake\"     \"vanilla tart\"     \"vanilla icecream\"\r\n#> [5] \"vanilla cookies\"  \"vanilla fudge\"    \"fruit\"    \"fruit tart\"   \"fruit sorbet\"    \r\n\r\nstr_replace_all(desserts, \"(dark )?chocolate\", \"vanilla\") \r\n#> [1] \"vanilla\"          \"vanilla cake\"     \"vanilla tart\"     \"vanilla icecream\" \r\n#> [5] \"vanilla cookies\"  \"vanilla fudge\"    \"fruit\"    \"fruit tart\"   \"fruit sorbet\"     \r\n\r\n\r\n\r\n\r\n\r\nbaser_vs_stringr <- microbenchmark(grep = grep(\".*\\\\bchocolate\\\\b.*\", desserts, value = F),\r\n                                   str_which = str_which(desserts, \".*\\\\bchocolate\\\\b.*\"),\r\n                                   gsub = gsub(\"chocolate\", \"vanilla\", desserts),\r\n                                   str_replace_all = str_replace_all(desserts, \"chocolate\", \"vanilla\"),\r\n                                   grepl = grepl(\".*\\\\bchocolate\\\\b.*\", desserts),\r\n                                   str_detect = str_detect(desserts, \".*\\\\bchocolate\\\\b.*\"),  \r\n                                   times = 1000)\r\n\r\nautoplot(baser_vs_stringr)  \r\n\r\n\r\n\r\n\r\nNote: Base R functions are significantly faster than their stringr equivalents.\r\nOther resources\r\nTips on regular expression usage are based on the excellent regular expressions vignette from stringr\r\nStrings chapter from R4DS by Garrett Grolemund and Hadley Wickham\r\nRStudio stringr cheatsheet\r\nhttps://regex101.com/ - a website for testing regular expressions\r\n\r\nMany R functions require R regex classes to be wrapped in a second set of [ ], e.g. [[:punct:]].↩︎\r\nAs a repetitive step within the workflow, we might want to rewrite the generic part of this cleaning step as a function stored in a separate R script for maximal readability.↩︎\r\n",
    "preview": "posts/2020-12-31-cleaning-free-text-and-wrangling-strings/benchmark.png",
    "last_modified": "2021-04-05T21:12:38+10:00",
    "input_file": "wrangling-strings-and-cleaning-free-text.utf8.md",
    "preview_width": 1949,
    "preview_height": 1200
  }
]
