---
title: "Introduction to data.table"
description: |
  To data.table or dplyr? That is the question.     
author: Erika Duan
date: 01-30-2021
preview: finalplot.png 
categories: 
  - data cleaning  
  - data.table  
  - dplyr
  - R  
output:
  distill::distill_article: 
    toc: true
---

```{r setup, include=FALSE}
# Set up global environment ----------------------------------------------------
knitr::opts_chunk$set(echo=TRUE, results='hide') 
options(scipen=999)
```


# Introduction   

Programming languages are still human constructs. They hold sway when they are utilised *en mass*, like Python for machine learning, and user factions may emerge if very different ways of doing the same thing concurrently exist.      

In R, this can manifest in the form of `data.table` versus `dplyr` debates. ^[I am extremely thankful for encountering these Twitter debates, as they helped draw more attention to `data.table` usage.]     

```{r, echo=FALSE, results='markup', fig.align='center', out.width='60%'} 
knitr::include_graphics("../../images/2021-01-30_twitter-post-data-table.jpg")
```

Both R packages contain a comprehensive stack of functions for data wrangling. The tidyverse `dplyr` approach emphasises code readability whilst `data.table` scales complex manipulations of large datasets very efficiently. You can compare the efficiency of `data.table` versus other data wrangling packages on large datasets [here](https://h2oai.github.io/db-benchmark/).    

Whilst I prefer to use `dplyr` on small datasets where `data.table` efficiency gains are negligible, I recommend using `data.table` when:      

+ You are using very large datasets (datasets over 0.5 million rows) **and**   
+ You need to use **group by** operations for data cleaning or feature engineering.         

Let's explore this for ourselves.            

```{r, message=FALSE, warning=FALSE}
# Load required packages -------------------------------------------------------  
if (!require("pacman")) install.packages("pacman")
pacman::p_load(here,
               lobstr, # Trace objects in memory 
               ids, # Generate random ids
               DT, # Create interactive tables  
               tidyverse,
               data.table,
               microbenchmark)
```


# Create a test dataset  

Imagine you have a dataset describing how students are engaging with online courses:     

+ Each student has a unique ID.   
+ There are 5 different online platforms, labelled **A**, **B**, **C**, **D** and **E**.   
+ Students have the option of taking different courses within the same platform or switching to a different platform.   
+ Platform start dates are recorded when the student starts the first course on a new platform.     
+ Platform end dates are recorded when the student exits a platform.    
+ Course start dates are recorded when the student starts a course.    
+ Course end dates are recorded when the student exits a course.    

**Note:** The code used to create this dataset can be accessed from my github repository [here](https://github.com/erikaduan/R-tips/blob/master/03_blog_posts/2020-04-07_data-table-versus-dplyr/2020-04-07_data-table-versus-dplyr.md).    

```{r, echo=FALSE}
# Create a function to generate random dates -----------------------------------
create_start_dates <- function(start_date, end_date, n) {
  # Assert requirement for character inputs 
  if(!is.character(start_date) | !is.character(end_date)) {
    stop("Error: start_date and end_date should be written in the format YYYY-mm-dd")
  }
  
  # Convert character inputs into dates
  start_date <- as.Date(start_date, format = "%Y-%m-%d")
  end_date <- as.Date(end_date, format = "%Y-%m-%d")  
  
  # Assert that end_date >= start_date  
  if(end_date < start_date) {
    stop("Error: start_date should be earlier than end_date")
  }
  
  # Generate date vector of random dates between start_date and end_date
  set.seed(111)
  sample(seq(start_date, end_date, by = "day"),
         n,
         replace = T)
}  
```

```{r, echo=FALSE}
# Create a function to populate course_end_dates -------------------------------
add_course_end_date <- function(df, include_n, row) {
  df %>% 
    group_by(join_key) %>% 
    mutate(course_end_date = if_else(n_courses >= include_n & row_number() == row,
                                     course_start_date + course_length,
                                     course_end_date)) %>%
    ungroup()
}
```

```{r, echo=FALSE}
# Create a function to populate course_start_dates -----------------------------
add_course_start_date <- function(df, include_n, row) {
  df %>% 
    group_by(join_key) %>% 
    mutate(course_start_date = if_else(n_courses >= include_n & row_number() == row,
                                       lag(course_end_date),
                                       course_start_date)) %>%
    ungroup()
}
```

```{r, echo=FALSE}
# Create 500000 student enrollments --------------------------------------------
# Create 150000 unique student IDs  
set.seed(111)
students <- random_id(n = 150000,
                      bytes = 4,
                      use_openssl = F) # So set.seed() works  

# Sample student IDs with replacement 500000 times 
id <- sample(students, 500000,
             replace = T) %>%
  sort()

# Simulate 5 platforms with different market shares  
platform <- sample(LETTERS[1:5], 500000,
                   replace = T,
                   prob = c(0.35, 0.05, 0.1, 0.3, 0.2)) 

# Create 20 courses 
all_courses <- c("R_beginner",
                 "R_intermediate",
                 "R_advanced",
                 "Python_beginner",
                 "Python_intermediate",
                 "Python_advanced",
                 "machine_learning",
                 "linear_algebra",
                 "statistics",
                 "UX_design",
                 "website_design",
                 "data_mining",
                 "travel_writing",
                 "bread_baking",
                 "pottery",
                 "poetry_writing",
                 "contemporary_dance",
                 "carpentry",
                 "metal_welding",
                 "fitness_training")  

course <- sample(all_courses, 500000,
                 replace = T)  

# Create student_courses ------------------------------------------------------- 
student_courses <- tibble(index = seq(1, 500000, 1), # index required for downstream joins
                          id,
                          platform,
                          course)
```

```{r, echo=FALSE}
# Create platform_start_date and platform_end_date entries----------------------
# Create join_key   
platform_subset <- student_courses %>%
  select(index, 
         id,
         platform) %>% 
  mutate(join_key = str_c(id, platform, sep = "-")) 

# Create lag_join_key  
platform_subset <- platform_subset %>%
  group_by(id) %>%
  mutate(lag_join_key = lag(join_key, 1)) %>%
  ungroup()  

# Filter rows representing the first course a student takes on a new platform      
platform_subset <- platform_subset %>%
  filter(is.na(lag_join_key) | lag_join_key != join_key)

# Create platform start_dates vector   
start_dates <- create_start_dates(start_date = "2016-01-01",
                                  end_date = "2019-01-01",  
                                  n = nrow(platform_subset))

platform_subset <- platform_subset %>%
  mutate(platform_start_date = start_dates)

# Create platform_end_date as platform_start_date + random platform_length 
set.seed(111)
platform_length <- runif(nrow(platform_subset),
                         min = 9, max = 90) %>%
  floor(.) 

# Round platform_length to a whole number i.e. whole day for date calculations   

platform_subset <- platform_subset %>%
  mutate(platform_end_date = platform_start_date + platform_length)  

# Left join student_course and provider_subset by index-------------------------
platform_subset <- platform_subset %>%
  select(index,
         platform_start_date,
         platform_end_date)     

student_courses <- left_join(student_courses,
                             platform_subset,
                             by = "index")

student_courses <- student_courses %>%
  fill(c(platform_start_date, platform_end_date), 
       .direction = "up") # NA inherits the value above    

student_courses <- student_courses %>%
  select(-index)   
```

```{r, echo=FALSE}
# Create course_start_date and course_end_date entries ------------------------- 
# Sort by id, platform start_date and platform   
student_courses <- student_courses %>%
  arrange(id, 
          platform_start_date,
          platform)

# Split student_courses into one_course and multiple_courses 
# Create join_key 
student_courses <- student_courses %>%
  mutate(platform_length = platform_end_date - platform_start_date,
         join_key = str_c(id, platform_start_date, platform, sep = "-"))

# Create lag_join_key 
student_courses <- student_courses %>%
  group_by(id) %>% 
  mutate(lag_join_key = lag(join_key)) %>%
  ungroup()

# Multiple courses exists when multiple rows share the same join_key     
multiple_courses_id <- student_courses %>%
  count(join_key) %>%
  filter(n > 1) %>%
  pull(join_key)

multiple_courses <- student_courses %>%
  filter(join_key %in% multiple_courses_id)

single_course <- student_courses %>%
  filter(!join_key %in% multiple_courses_id)

# Create course date logic for single_course------------------------------------  
single_course <- single_course %>%
  mutate(course_start_date = platform_start_date,
         course_end_date = platform_end_date) 

# Create course date logic for multiple_courses---------------------------------
# Check maximum number of courses undertaken on the same platform
multiple_courses %>%
  count(join_key) %>% 
  ungroup() %>%
  pull(n) %>%
  max()  
#> [1] 7

# Populate first course_start_date and last course_end_date for all cases    
set.seed(111)
multiple_courses <- multiple_courses %>%
  group_by(join_key) %>%
  mutate(n_courses = n(),
         course_start_date = if_else(row_number() == 1, platform_start_date,
                                     NA_real_),
         course_end_date = if_else(row_number() == n(), platform_end_date,
                                   NA_real_),
         course_length = runif(n = n(), min = 1, max = platform_length / n_courses) %>% ceiling()) %>%
  ungroup()

# Populate first course_end_date for all cases  
multiple_courses <- multiple_courses %>% 
  group_by(join_key) %>% 
  mutate(course_end_date = if_else(!is.na(course_start_date),
                                   course_start_date + course_length,
                                   course_end_date)) %>%
  ungroup()

# Populate second course_start_date when n_courses >= 2
multiple_courses <- add_course_start_date(multiple_courses, include_n = 2, row = 2)

# Populate second course_end_date when n_courses >= 3  
multiple_courses <- add_course_end_date(multiple_courses, include_n = 3, row = 2)

# Populate third course_start_date when n_courses >= 3  
multiple_courses <- add_course_start_date(multiple_courses, include_n = 3, row = 3)

# Populate third course_end_date when n_courses >= 4  
multiple_courses <- add_course_end_date(multiple_courses, include_n = 4, row = 3)

# Populate fourth course_start_date when n_courses >= 4  
multiple_courses <- add_course_start_date(multiple_courses, include_n = 4, row = 4)

# Populate fourth course_end_date when n_courses >= 5  
multiple_courses <- add_course_end_date(multiple_courses, include_n = 5, row = 4)

# Populate fifth course_start_date when n_courses >= 5  
multiple_courses <- add_course_start_date(multiple_courses, include_n = 5, row = 5)

# Populate fifth course_end_date when n_courses >= 6  
multiple_courses <- add_course_end_date(multiple_courses, include_n = 6, row = 5)

# Populate sixth course_start_date when n_courses >= 6  
multiple_courses <- add_course_start_date(multiple_courses, include_n = 6, row = 6)

# Populate sixth course_end_date when n_courses >= 7  
multiple_courses <- add_course_end_date(multiple_courses, include_n = 7, row = 6)

# Populate seventh course_start_date when n_courses >= 7  
multiple_courses <- add_course_start_date(multiple_courses, include_n = 7, row = 7)

# Bind all rows and re-order student_courses -----------------------------------    
student_courses <- bind_rows(single_course,
                             multiple_courses)

student_courses <- student_courses %>%
  select(-c(platform_length,
            course_length,
            n_courses,
            join_key,
            lag_join_key))  

student_courses <- student_courses %>%
  arrange(id,
          platform_start_date,
          platform)
```

```{r, echo=FALSE}
# Remove all objects except student_courses-------------------------------------  
rm(list = setdiff(ls(), "student_courses"))
gc()
```

We can interactively examine the first 20 rows of the dataset using the R package `DT`.      

```{r, echo=FALSE, results='markup'}
# Examine the first 12 rows of data--------------------------------------------  
student_courses %>%
  head(12) %>%
  datatable(rownames = F,
            options = list(pageLength = 6, dom = 'tip',
                           initComplete = JS(
                             "function(settings, json) {",
                             "$(this.api().table().header()).css({'background-color': '#37ACA1', 'color': '#fff'});",
                             "}")))
```


# Principles of `data.table`     

In R, datasets exist as `data.frame` type objects. To apply `data.table` functions on a dataset, we need to convert a `data.frame` into a `data.table` object using `setDT()`.      

This function is flexible as it converts a `data.frame` by reference (i.e. without creating a duplicate `data.table` copy) and assigns both `data.table` and `data.frame` classes to the converted object.    

```{r}
# Convert data frame into data.table -------------------------------------------  
class(student_courses)
#> [1] "tbl_df"     "tbl"        "data.frame"

mem_used()
#> 222,639,352 B

# Track object assignment in memory  
tracemem(student_courses)   
#> [1] "<0000022B07B7AB70>"   

setDT(student_courses) # data.table is assigned to a new location in memory   
#> tracemem[0x0000022b07b7ab70 -> 0x0000022b047a8478]: as.list.data.frame as.list vapply vapply_1i setDT   

untracemem(student_courses)   

mem_used()     
#> 242,674,176 B  

# Note that computer memory has not doubled following setDT()  

class(student_courses)
#> [1] "data.table" "data.frame"    
```


## `data.table` query structure

A `data.table` query is structured in the form `DT[i, j, by]` where:  

+ Data selection (i.e. filtering or sorting rows) is performed in the `i` placeholder.    
+ Data column selection or creation is performed in the `j` placeholder.       
+ Grouping data by variable(s) is performed in the `by` placeholder.        

```{r, echo=FALSE, results='markup', fig.align='center', out.width='80%'} 
knitr::include_graphics("../../images/2021-01-30_DT-query-structure.jpg")
```


## `data.table` efficiency gains    

There are a few reasons why `data.table` operations are fast:  

+ Many of its operations, including grouping, reading and writing, are parallelised by default.   
+ `data.table` automatically creates a secondary index (or key) of the columns used to subset data, so that subsequent operations on the same column are much faster.  
+ `data.table` has a much faster `order()` function, which is also utilised for the evaluation of groupings.  
+ You can use the operator `:=` to add, delete or modify columns in place, which is a faster alternative to R's default copy-on-modify behaviour.     

```{r}
# Create data.frame and data.table objects ------------------------------------- 
df <- data.frame(id = seq(1:5),
                 letter = letters[1:5])

dt <- as.data.table(df)  

# Update data.frame column using copy-on-modify -------------------------------- 
df_2 <- df %>%
  mutate(letter = toupper(letter)) 

# The new data frame is a shallow copy of the original data frame as
# only modified columns are newly created in memory.       

ref(df, df_2)
#> o [1:0x22b03b6c998] <df[,2]> 
#> +-id = [2:0x22b7f3f6350] <int> 
#> \-letter = [3:0x22b0c54b328] <chr> 
 
#> o [4:0x22b03b90e78] <df[,2]> 
#> +-id = [2:0x22b7f3f6350] 
#> \-letter = [5:0x22b05f9b890] <chr> 

# Update data.table column by reference ----------------------------------------
obj_addr(dt)
#> [1] "0x22b016ed030"

dt[, letter := toupper(letter)]

obj_addr(dt)
#> [1] "0x22b016ed030"  
```

```{r, results='markup'}
dt # Check that dt has been modified in place      
```

**Note:** You do not need to assign datasets to names when modifying by reference using the `:=` operator.         


# Filter data     


## Using `dplyr` versus `data.table`     

The syntax for filtering data is very similar for `dplyr` and `data.table`. 

```{r}
# Filter student_courses using dplyr -------------------------------------------    
# Filter by platform A 
student_courses %>%
  filter(platform == "A")  

# Filter by all platforms excepting A  
student_courses %>%
  filter(platform != "A") 

# Filter by platforms A and C    
student_courses %>%
  filter(platform %in% c("A", "C")) 

# Using a comma is a substitute for the condition 'and' 
student_courses %>%
  filter(id == "00007f23", 
        between(platform_start_date, "2017-01-01", "2017-12-31"))

# Filter by a variable using regex   
student_courses %>%
  filter(str_detect(course, "^R_")) 
```

A difference is that `data.table` also contains a list of helper functions with optimised performance for filtering on specific data types like characters or integers.       

```{r}
# Filter student_courses using data.table --------------------------------------  
# Filter by platform A 
student_courses[platform == "A"] 

# Filter by all platforms excepting A   
student_courses[platform != "A"] 

# Operator %chin% is equivalent to but much faster than %in%  
student_courses[platform %chin% c("A", "C")] 

# Operator %between% allows you to search for values in a closed inclusive interval 
student_courses[id == "00007f23" & platform_start_date %between% c("2017-01-01", "2017-12-31")]  

# Operator %like% allows you to search for a pattern in a character vector 
student_courses[course %like% "R_"]  
```


## Benchmark data filtering   

You can use the R package `microbenchmark` to measure code performance. ^[For accurate benchmarking, you need to separately run, save and print `microbenchmark()` outputs rather than directly knitting results.]        

The function `microbenchmark()` runs each expression 100 times by default with the argument `times = 100L`. It outputs summary statistics on how long it takes to evaluate a single expression.   

```{r, echo=FALSE, results='markup'}
# Benchmark dplyr and data.table functions -------------------------------------  
if (!file.exists(here("data", "data-table-benchmarks", "fmb.rds"))) {
  
  fmb <- microbenchmark(filter(student_courses, platform == "A"),
                        student_courses[platform == "A"],
                        filter(student_courses, platform %in% c("A", "C")),
                        student_courses[platform %chin% c("A", "C")],
                        filter(student_courses, str_detect(course, "^R_")),
                        student_courses[course %like% "R_"]) 
  
  saveRDS(fmb, here("data", "data-table-benchmarks", "fmb.rds"))
}

# Print summary table ----------------------------------------------------------
fmb <- readRDS(here("data", "data-table-benchmarks", "fmb.rds"))
knitr::kable(summary(fmb), caption = "Units: milliseconds")    
```


# Sort data    


## Using `dplyr` versus `data.table`    

Sorting a data frame can be computationally expensive when multiple variables need to be ranked. This is why I recommending sorting your dataset once, right after basic data cleaning operations have been performed.         

```{r}
# Sort student_courses using dplyr ---------------------------------------------   
student_courses %>%
  arrange(course_start_date) 

student_courses %>%
  arrange(platform,
          id, 
          desc(platform_start_date)) 

# Sorting by a descending date ranks the most recent date as first  
```

In `data.table`, sorting is also performed inside `i` of `DT[i, j, by]`. Using the operator `-` in front of a variable allows sorting by descending order.     

```{r}
# Sort student_courses using data.table ---------------------------------------- 
student_courses[order(course_start_date)]  

student_courses[order(platform,
                      id,
                      -platform_start_date)]  

# You can also order columns in place using setorder()
setorder(student_courses,
         id,
         platform_start_date,
         course_start_date)  
```


## Benchmark data sorting   

You can see that `order()` from `data.table` sorts data much faster than its equivalent `dplyr` function.    

```{r, echo=FALSE, results='markup'}
# Benchmark dplyr and data.table functions -------------------------------------  
if (!file.exists(here("data", "data-table-benchmarks", "smb.rds"))) {
  
  smb <- microbenchmark(arrange(student_courses, course_start_date),
                        student_courses[order(course_start_date)],
                        arrange(student_courses, platform, id, desc(platform_start_date)),  
                        student_courses[order(platform, id, -platform_start_date)],
                        times = 25)
  
  saveRDS(smb, here("data", "data-table-benchmarks", "smb.rds"))
}

# Print summary table ----------------------------------------------------------
smb <- readRDS(here("data", "data-table-benchmarks", "smb.rds"))
knitr::kable(summary(smb), caption = "Units: milliseconds")
```


# Select data columns    


## Using `dplyr` versus `data.table`  

In `dplyr`, performing operations on a tibble will always return another data frame, unless you explicitly use `pull()` to extract a column as a vector.   

```{r}
# Select column(s) using dplyr -------------------------------------------------   
student_courses %>%
  select(id)

student_courses %>%
  select(c(id, platform, course))

# Select columns(s) using regex  
student_courses %>%
  select(contains("date", ignore.case = F))

# Output data.frame with select() ----------------------------------------------
student_courses %>%
  select(id) %>%
  class()
#> [1] "data.table" "data.frame"  

# Output vector with pull() ----------------------------------------------------
student_courses %>%
  pull(id) %>%
  class()
#> [1] "character"
```

In `data.table`, column selection is performed inside `j` of `DT[i, j, by]` and returns a `data.table` if you wrap column names inside a list.   

```{r}  
# Select column(s) using data.table --------------------------------------------  
student_courses[,
                .(id)]

student_courses[,
                .(id,
                  platform,
                  course)]

# Select column(s) using regex  
grep("date", names(student_courses), value = T)
#> [1] "platform_start_date" "platform_end_date" 

student_courses[,
                grep("date", names(student_courses), value = T), 
                with = F]

# Output data frame by wrapping column names inside a list ---------------------  
class(student_courses[, .(id)])
#> [1] "data.table" "data.frame"   

# Output vector with [[x]] -----------------------------------------------------
class(student_courses[, id])  
#> [1] "character"

class(student_courses[["id"]])  
#> [1] "character"
```   


## Benchmark column selection     

Interestingly, the benchmark below shows that `dplyr` is slightly faster than `data.table` for column selections.   

```{r, echo=FALSE, results='markup'}
#Benchmark dplyr and data.table functions --------------------------------------  
if (!file.exists(here("data", "data-table-benchmarks", "slmb.rds"))) {
  
  slmb <- microbenchmark(select(student_courses, c(id, platform, course)),   
                         student_courses[, .(id, platform, course)],
                         select(student_courses, contains("date", ignore.case = F)), 
                         student_courses[, grep("date", names(student_courses), value = T), with = F],
                         times = 100)  
  saveRDS(slmb, here("data", "data-table-benchmarks", "slmb.rds")) 
} 

# Print summary table ----------------------------------------------------------
slmb <- readRDS(here("data", "data-table-benchmarks", "slmb.rds"))   
knitr::kable(summary(slmb), caption = "Units: milliseconds")    
```


# Column creation      


## Using `dplyr` versus `data.table`  

In `dplyr` version >= 1.0.0, you can use `mutate()` in combination with `across()` to create new columns or apply transformations across one or multiple columns.   

A shallow `data.frame` copy is created whenever a column is modified, which you then assign a name to.   

```{r}
# Create column(s) using dplyr -------------------------------------------------  
# Create new columns from existing variables   
student_courses <- student_courses %>%
  mutate(platform_dwell_length = platform_end_date - platform_start_date,
         platform_start_year = as.numeric(str_extract(platform_start_date, "^.{4}(?!//-)")))

# Create column(s) with multiple conditions using dplyr ------------------------  
str_subset(unique(student_courses$course), "^R_")
#> [1] "R_beginner"     "R_advanced"     "R_intermediate"
str_subset(unique(student_courses$course), "^Python_")
#> [1] "Python_intermediate" "Python_advanced"     "Python_beginner"    

student_courses <- student_courses %>%
  mutate(studied_programming = case_when(str_detect(course, "^R_") ~ "Studied R",
                                         str_detect(course, "^Python_") ~ "Studied Python",
                                         TRUE ~ "No"))    

# Remove newly created columns using dplyr -------------------------------------  
student_courses <- student_courses %>%
  select(-c(platform_dwell_length,
            platform_start_year, 
            studied_programming))
```

Data frame outputs are slightly different in `data.table`:     

+ Column transformations are always modified in place using the operator `:=`.    
+ Multiple columns can be concurrently modified, as long as the newly created columns do not depend on each other.    
+ Use subassignment to only extract the columns transformed inside `j` of `DT[i, j, by]`.    

```{r}
# Create column(s) using data.table --------------------------------------------  
student_courses[,
                `:=` (platform_dwell_length = platform_end_date - platform_start_date,
                      platform_start_year = str_extract(platform_start_date, "^.{4}(?!//-)"))] 

student_courses[,
                platform_start_year := as.numeric(platform_start_year)]  

# Create column(s) with multiple conditions using data.table -------------------
student_courses[,
                studied_programming := fcase(
                  str_detect(course, "^R_"), "Studied R",
                  str_detect(course, "^Python_"), "Studied Python",
                  default = "No")]  

# Remove newly created columns using data.table --------------------------------
# Column(s) can be removed by assignment as NULL variable(s)  
student_courses[,
                c("platform_dwell_length",
                  "platform_start_year",
                  "studied_programming") := NULL]
```

```{r}
# Only columns transformed inside j are kept ----------------------------------- 
# Use subassignment only if you want to subset the transformed columns     
platform_records <- student_courses[,
                                    .(id = toupper(id),
                                      platform = tolower(platform))]  

ncol(platform_records)
#> [1] 2  
```


## Benchmark column creation            

```{r, echo=FALSE, results='markup'}
# Benchmark dplyr and data.table functions -------------------------------------  
if (!file.exists(here("data", "data-table-benchmarks", "ccmb.rds"))) {
  
  ccmb <- microbenchmark(mutate(student_courses,
                                platform_dwell_length = platform_end_date - platform_start_date),
                         student_courses[, "platform_dwell_length" := platform_end_date - platform_start_date], 
                         dplyr_case_when = mutate(student_courses,
                                                  studied_programming = case_when(str_detect(course, "^R_") ~ "Studied R",
                                                                                  str_detect(course, "^Python_") ~ "Studied Python",
                                                                                  TRUE ~ "No")),    
                         data.table_fcase = student_courses[, studied_programming := fcase(
                           str_detect(course, "^R_"), "Studied R",
                           str_detect(course, "^Python_"), "Studied Python",
                           default = "No")], 
                         times = 100)  
  
  saveRDS(ccmb, here("data", "data-table-benchmarks", "ccmb.rds"))
}

# Print summary table ----------------------------------------------------------
ccmb <- readRDS(here("data", "data-table-benchmarks", "ccmb.rds"))
knitr::kable(summary(ccmb), caption = "Units: milliseconds")   
```


# Simple group by operations    


## Using `dplyr` versus `data.table`  

Evaluating summary characteristics for data groupings is also where `data.table` significantly outperforms `dplyr`. A grouping is specified using the `group_by()` function in `dplyr` and inside the `by` placeholder of `DT[i, j, by]` in `data.table`.      

```{r}
# Find total number of courses per student via dplyr ---------------------------    
student_courses %>%
  group_by(id) %>%
  summarise(total_courses = n()) %>%
  ungroup()

# Code above is also equivalent to using count()   
student_courses %>%
  count(id)

# Find total number of distinct courses per student via dplyr ------------------   
student_courses %>%
  group_by(id) %>%
  summarise(total_distinct_courses = n_distinct(course)) %>%
  ungroup() 

# Find the first course studied per student and platform via dplyr -------------
student_courses %>%
  group_by(id, platform) %>% # Group by two variables    
  filter(row_number() == 1L) %>% # Return the first row from each group    
  ungroup()   
```

```{r}
# Find total number of courses per student via data.table ----------------------    
student_courses[,
                .(total_courses = .N),
                by = id]   

# Find total number of distinct courses per student via data.table -------------   
student_courses[,
                .(total_distinct_courses = length(unique(course))),
                by = id]   

# uniqueN(x) is a data.table function equivalent to length((unique(x)) 
student_courses[,
                .(total_distinct_courses = uniqueN(course)),
                by = id]   

# Find the first course studied per student and platform via data.table --------
student_courses[,
                .SD[1L],
                by = .(id, platform)]
```


## Benchmark simple group by operations    

You can clearly see that groupings are significantly faster in `data.table` than `dplyr`.    

**Note:** Not all `data.table` functions outperform their base R or `dplyr` equivalents. The `data.table` `uniqueN(x)` function is much slower than `length(unique(x))`.    

```{r, echo=FALSE, results='markup'}
# Benchmark dplyr and data.table functions -------------------------------------  
if (!file.exists(here("data", "data-table-benchmarks", "gbmb.rds"))) {
  
  gbmb <- microbenchmark(student_courses %>% group_by(id) %>% summarise(total_courses = n()),                                                         student_courses[, .(total_courses = .N), by = id],    
                         student_courses %>% group_by(id) %>% summarise(total_distinct_courses = n_distinct(course)),
                         student_courses[, .(total_distinct_courses = length(unique(course))), by = id],    
                         student_courses[, .(total_distinct_courses = uniqueN(course)), by = id],      
                         student_courses %>% group_by(id, platform) %>% filter(row_number() == 1L),  
                         student_courses[, .SD[1L], by = .(id, platform)],   
                         times = 10)  
  
  saveRDS(gbmb, here("data", "data-table-benchmarks", "gbmb.rds"))
}

# Print summary table ----------------------------------------------------------
gbmb <- readRDS(here("data", "data-table-benchmarks", "gbmb.rds"))
knitr::kable(summary(gbmb), caption = "Units: milliseconds") 
```


# Summary   

Most `data.table` operations significantly outperform their `dplyr` equivalents in processing speed. I use `data.table` when grouping on large datasets (i.e. on datasets with greater than ~ 0.5 million rows) and use `dplyr` for day-to-day analyses of smaller datasets.   


# Other resources    

+ A [stack overflow discussion](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly/27840349#27840349) about the best use cases for `data.table` versus `dplyr`.     

+ A great side-by-side comparison of `data.table` versus `dplyr` functions from a [blog post by Atrebas](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/).            

+ A list of advanced `data.table` operations and tricks from a [blog post by Andrew Brooks](http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/).        

+ An explanation of how `data.table` modifies by reference from a [blog post by Tyson Barrett](https://tysonbarrett.com//jekyll/update/2019/07/12/datatable/).     

+ A benchmark of `dplyr` versus `data.table` functions from a [blog post by Tyson Barrett](https://tysonbarrett.com/jekyll/update/2019/10/06/datatable_memory/)    